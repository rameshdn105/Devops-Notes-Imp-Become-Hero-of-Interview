Docker Commands:

1. $$ docker –version    : This command is used to get the currently installed version of docker
2. $$ docker ps : list the running containers
3. $$ docker ps -a : show all the running and exited containers
4. $$ docker container ls -s : to view approximate size of a running container
3. $$ docker pull <image name>	:This command is used to pull images from the docker repository(hub.docker.com)
4. docker build: To create a image from dockerfile
   $$ docker build -t <imagename>:<tag> <path of the Dockerfile>
10. docker commit: To create a image from a existing container
   $$ docker commit <container id> <imagename>:<tag>
5. docker run: This command is used to run a container from an image
   $$ docker run -it -d <image name>:tag
6. docker attach: get into container /login to container
   $$ docker attach <container id>
7. docker exec: used to access/login the running container (inorder to run in the session of bin bash )
   $$ docker exec -it <container ID> /bin/bash
8. docker stop: stops a running container
   $$ docker stop <container id>
9. docker kill: This command kills the container by stopping its execution immediately
   $$ docker kill <container id>
10.docker rm: to remove/delete a stopped container
   $$ docker rm <containerid>  (forcefully *docker rm -f <containerid>)
   $$ docker rm $(docker stop <contaner id>)
10.docker login: login to the docker hub repository
   $$ docker tag <local-image>:<tag> <hub_username>/<reponame>:<tag>   --->before pushing the image we need to tag the image properly
   $$ docker push <hub_username>/<repo_name>:<tag>		(login required)
-> docker pull centos/alpine/wordpress/archlinux
-> pulling images for centos (installing centos) from docker image registry(called docker hub), to create and run a container
* $$ docker stop <containerID>
* $$ docker start <containerID>
* $$ docker stats <containerID> 
* $$ docker inspect <containerID>: 
* docker info   : it gives the information of the docker, like no of containers, kernel version, kernal name
* flags -p: assign port target
        -d: detached mode
        -v: attach volume
        -q: quit
        -f: filter 

11.$$ docker images: lists all the locally stored docker images
12.docker rmi: to delete an image from local storage
   $$ docker rmi <image-id>
   $$ docker image rm <imagename>:tag  Ex:nginx:alpine

13.$$ docker volume create <volume name>---> to create docker volumes
14.$$ docker volume ls --->list all the volumes
15.$$ docker volume rm <volume name> ----> to delete docker volume
16.$$ docker volume prune   ----> delete unused volumes
17.$$ docker volume inspect <volume name> ---> to display the details about volume
18.$$ docker run -v <volume name>:<continaer path>  --> attaching volume
   $$ docker run -it -v vol1:/root/vol1 --name <image name> ubuntu   : mount container to the volume, or attach container to volume

19.$$ docker network create --driver bridge <network_name>   --->create a custom bridge network
20.$$ docker run -dti --network <network_name><imagename:tag>  --->create a container in the custom bridge
21.Connect containers present in diff bridge
   $$ docker network connect <network_name> <container id>
   $$ docker network disconnect <network_name> <container id>
22.$$ docker run -d -it --network host <imagename:tag>   -->running a host network
23.To create an overlay network for use with swarm services: 
   $$ docker network create -d overlay my-overlay
   -> To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag:
   $$ docker network create -d overlay --attachable my-attachable-overlay

24.to publish a single port 
   $$ docker run -p <hostport>:<container_port> .....
   to publish all the available exposed port of the container 
   $$ docker run -P ..............
25.container logs: for retrieving container logs
  $$ sudo docker container logs [option] container_id
26.container logs: all logs of a docker container
  $$ docker ps -q | xargs -L 1 docker logs
27.docker memory: To limit the maximum amount of memory usage for a container, add the --memory/-m
  $$ sudo docker run -it --memory=" [memory_limit]" [docker_image]

28. Docker compose commands
docker compose up -d :to start docker compose file
docker compose logs-->to check logs generated by the compose
docker compose pause--->If you want to pause the environment execution without changing the current state of your containers
               unpause
docker compose stop--->The stop command will terminate the container execution, but it won’t destroy any data associated with your containers
docker compose down--->If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command

-------------------------------------------------------------------------------------------------------------------------------
Q. daemon login and container login
-> Daemon Login: In computing, a daemon is a background process that runs on a computer and performs specific tasks. 
   A Daemon Login is a type of login in which a user is granted access to the computer as a daemon process, allowing them 
   to run specific tasks in the background. This type of login is typically used by system administrators to perform 
   administrative tasks, such as starting and stopping services or running scripts.
-> Container Login: In the context of containers, a Container Login is the process of logging into a container to 
   access its environment and run commands or make changes. This is typically done by using a command line interface 
   (CLI) such as Docker or Kubernetes. 

Q. restart policies for docker container with example?
-> It tells Docker how to handle restart a container that has stopped.
    Here are the available restart policies:
1. No: Never attempt to restart the container. (
	$$ docker run --name my_container --restart=no my_image
2. On-failure: Restart the container only if it stops due to an error, and add a maximum restart count.
	$$ docker run --name my_container --restart=on-failure:5 my_image
3. Always: Always restart the container regardless of the exit status.
	$$ docker run --name my_container --restart=always my_image
4. Unless-stopped: Restart the container always, unless it is explicitly stopped.
	$$ docker run --name my_container --restart=unless-stopped my_image

Q. docker layer
-> the fundamental building blocks for creating, deploying, and scaling systems.
-> Docker stores all caches in /var/lib/docker/<driver> , where <driver> is the storage driver overlay2 again.

Q. What is the difference containerd and runC?
-> containerd is called a high-level container runtime. For some actions, it makes use of yet another runtime, 
   called a low-level container runtime. This low-level runtime is called runc

Q. What is docker system prune?
-> docker system prune is not safe to be used in production. It may clean up and reclaim space but there's a possibility that one 
   or more containers will die and need to be restarted manually. In production apps, you want the images to fail gracefully and 
   restart automatically.

Q. What are orphaned containers?
-> Orphaned containers are basically containers you used previously, but you deleted the dependencies to 
   them so you can't use them anymore, but they are still present on your computer.
  $$ docker-compose up -d --remove-orphans

Q. Docker inspect: provides detailed information on constructs controlled by Docker, render results in a JSON array
 $ docker volume ispect

Q. Diff between running and creating the container?
-> Start will start any stopped containers. This includes freshly created containers. 
   Run is a combination of create and start. It creates the container and starts it.
-> ARG will be available only while building the image. while ENV variables are availble while building and also while running the container.

Q. Difference between Docker kill and Docker stop:
-> To terminate a container, Docker provides the docker stop and docker kill commands. 
   Both the docker kill and docker stop commands look similar, but their internal execution is different. 
   The docker stop commands issue the SIGTERM signal to the main process running, whereas the docker kill commands sends the SIGKILL signal to the process.
-> ‘docker stop’ gives the container time to shutdown gracefully, in situations when it is taking too much time for getting the container to stop, one can opt to kill it

Q. The three primary differences between the Dockerfile and docker-compose are:
-> The Dockerfile is used to build images while the docker-compose.yaml file is used to run images.
-> The Dockerfile uses the docker build command, while the docker-compose.yaml file uses the docker-compose up command.
-> A docker-compose.yaml file can reference a Dockerfile, but a Dockerfile can’t reference a docker-compose file.

Q. how to login to docker repository?
-> You can log into any public or private repository for which you have credentials. 
   When you log in, the command stores credentials in $HOME/. docker/config. json on Linux or %USERPROFILE%/.

Q. Difference between docker image and layer?
-> A Docker image is a package that contains all the files and dependencies needed to run a specific application or service. 
   It is built from one or more layers, which are stacked on top of each other to create the final image.
-> A Docker layer, also known as an image layer, is a set of changes to the file system that make up a specific version of an image. 
   Each layer is represented by a unique identifier, called a "digest," which is generated by a hash of the contents of the layer.
-> Image is the final product, while a layer is a building block used to create the image.
---------------------------------------------------------------------------------------------------------------

*Assignment- Dangling images and how to delete them.
Ans: Simply an unused image that’s got no name and tag. You can easily spot dangling images when you run the docker 
images command because they show up as <none>:<none>.

*docker system prune :Remove all dangling images. If -a is specified, will also remove all images not referenced by any container.
--all , -a		Remove all unused images, not just dangling ones
--filter		Provide filter values (e.g. 'until=<timestamp>')
--force , -f		Do not prompt for confirmation

List danglin images:
$ sudo docker images -f dangling=true

*How Are Dangling Images Created?
-> Dangling images are usually created when an existing image gets superseded by a new build.
*Can You Use a Dangling Image?
-> Dangling images function like any other image. The only difference is the missing tag. 
   You can start a container from a dangling image by directly referencing the image’s ID.
*Cleaning Up Dangling Images
-> You can delete a single dangling image using the docker rmi command, just like any other image. 
   Because the image won’t be tagged, you’ll need to identify it by its ID.
-> Docker image prune :Remove all dangling images.
-----------------------------------------------------------------------------------------------------------------------------------------------------

Why Docker
----------------
Suppose there are four developers in a team working on a single project.
Meanwhile, one is having a Windows system, the second is owning a Linux system, 
and the third & fourth ones are working with macOS. Now, as you see, 
they are using the distinct environments for creating a single application or software they will be 
required to carry on the things in accordance with their respective machines such as the installation 
of different libraries & files for their system, etc. And such situations, especially on an organizational 
or larger level, often cause numerous conflicts and problems throughout the entire software development life cycle.
However, the containerization tools such as Docker eliminates this problem.

--------------------------------------------------------------------------------------------------------------------------------------------------------
*Tomcat: Tomcat is widely used by web developers when working on web application development. 
 From a high-level perspective, apache tomcat is responsible to provide a run-time environment for the servlets. 
 It provides an environment in which one could run their java code.

->Tomcat: Born out of the Apache Jakarta Project, Tomcat is an application server designed to execute Java servlets and render web pages that 
  use Java Server page coding. Accessible as either a binary or a source code version, Tomcat’s been used to power a wide range of applications 
  and websites across the Internet.

*Catalina.sh:
It is the script that is actually responsible for starting Tomcat; the "startup" script simply runs "catalina" with the argument "start" 
("catalina" also can be used with the "stop" parameter to shut down Tomcat).

*Start up.sh: Located in the /base/scripts directory, the startup script (startup.sh) is run by the system boot process. 
*Near the end of startup.sh is a list of services that will be run upon startup.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker (Docker --Version: 20.10.12): 
-> Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. 
   Containers allow a developer to package up an application with all of the parts it needs, such as 
   libraries and other dependencies, and ship it all out as one package.

*Pre-requisite for docker: Ubuntu server setup with non sudo root user & a firewall, docker hub account
------------------------------------------------------------------------------------------------------------------------------------------------------------

Containerization and virtualization:
-> Virtualization aims to run multiple OS instances on a single server, whereas containerization runs a single OS instance, with multiple user spaces 
   to isolate processes from one another.
-> This means containerization makes sense for one AWS cloud user that plans to run multiple processes simultaneously.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Diffrence between VM and container.
The key differentiator between containers and virtual machines is that virtual machines virtualize an entire machine down to the hardware layers
and containers only virtualize software layers above the operating system level.
	          		Docker	                                                      Virtual Machines (VMs)
Boot-Time		Boots in a few seconds.	                                	It takes a few minutes for VMs to boot.
Runs on	        	Dockers make use of the execution engine/docker engine.	       	VMs make use of the hypervisor.
Memory Efficiency    	No space is needed to virtualize, hence less memory. 		Requires entire OS to be loaded before starting the surface, so less efficient. 
Deployment		Deploying is easy as only a single image, 			Deployment is comparatively lengthy as separate instances 
               		containerized can be used across all platforms.   		are responsible for execution.		

-> A hypervisor, also known as a virtual machine monitor or VMM, is software that creates and runs virtual machines (VMs). 
  A hypervisor allows one host computer to support multiple guest VMs by virtually sharing its resources, such as memory 
  and processing.
Types:
1. Type 1 (Bare metal) : acts like a lightweight operating system and runs directly on the host's hardware
2. Type 2 (Hosted) :  runs as a software layer on an operating system, like other computer programs.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker architecture:
https://docs.docker.com/get-started/overview/
Docker uses a client-server architecture. 
*The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. 
*The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. 
*The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. 
*Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.

The Docker client: 
-> The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as 
   docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. 
   The Docker client can communicate with more than one daemon.

The Docker daemon: 
-> The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. 
   A daemon can also communicate with other daemons to manage Docker services.

Docker Desktop:
-> Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and 
   share containerized applications and microservices. Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), 
   Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop.

Docker registries:
-> A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for 
   images on Docker Hub by default. You can even run your own private registry.
-> When you use the docker pull or docker run commands, the required images are pulled from your configured registry. 
   When you use the docker push command, your image is pushed to your configured registry. (To push we login to dockerhub)
-> Private registry: Amazon ECS(Elastic container service), Docker enterprise, Azure Kuberntes service, Google Kubernetes Engine (GKE)
-> other regestreis: Google Container Registry, Amazon Elastic Container Registry, Azure Container Registry, GitLab Container Registry, 
   JFrog Container Registry, Quay, Harbor.

Docker Engine overview:
-> Docker Engine is an open source containerization technology for building and containerizing your applications. 
   Docker Engine acts as a client-server application with:
* A server with a long-running daemon process dockerd.
* APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
* A command line interface (CLI) client docker.
-> The CLI uses Docker APIs to control or interact with the Docker daemon through scripting or direct CLI commands. 
   Many other Docker applications use the underlying API and CLI. 
   The daemon creates and manage Docker objects, such as images, containers, networks, and volumes.

-> Docker container: A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly 
   and reliably from one computing environment to another. 
-> A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: 
   code, runtime, system tools, system libraries and settings.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker Container Lifecycle Management or stages of container:
1. Created state : docker create --name <name-of-container> <docker-image-name>
2. Running state : docker run <container-id or container-name> 
3. Paused state/unpaused state: docker pause container <container-id or container-name> & docker unpause <container-id or container-name>
4. Stopped state: docker stop <container-id or container-name>
5. Killed/Deleted state : docker kill <container-id or container-name>

---------------------------------------------------------------------------------------------------------------------------------------------------
Install docker
--------------------------------------------
https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-22-04
to run without sudo user use-

sudo usermod -aG docker ubuntu   ----->adding ubuntu into docker group
sudo systemctl enable docker   ----->starts docker whenever machine starts

docker run -it -p 8080:8080 jenkins/jenkins:lts    ------>docker command to run jenkins
it: interactive terminal

*Assignment install sonarqube using docker--->https://techexpert.tips/sonarqube/sonarqube-docker-installation/

---------------------------------------------------------------------------------------------------------------------------------------------------
Docker commands:
*docker stats [OPTIONS] [CONTAINER...]  -->Display a live stream of container(s) resource usage statistics.
*docker info --> displays system wide information regarding the Docker installation (kernel version, number of containers and images)
*dockr ps-->list the running containers
*docker container ls---> to list the running containers
*docker ps -q   :(--quiet only the container IDs)
*docker ps -a -->list all the containers
*docker	image ls
*docker	volumes ls
*docker	networks ls
*docker ps -f status=exited --> list only exited containers or stopped containers   (-f // --filter)
*docker ps -f status=exited | xargs -l {}docker rm                    alternate command docker rm $(docker ps -q -f status=exited)
docker save <container name>  : to save file to local desktop
docker load <container name> : to reload it back to 
*docker start $(docker ps -a -q --filter "status=exited")

---------------------------------------------------------------------------------------------------------------------------------------------------
Best way To delete all the running containers
--> stop all running containers
*docker stop $(docker ps -q)
-->then delete the stopped containers
*docker rm $(docker stop $(docker ps -q))

*docker rm $(docker stop <contaner id>)

Automatically delete the container when it stopped
*docker run --rm 

To create a container in background /in detached mode (If we dont run in detach mode we can run commands)
*docker run -d <imagename>:<tagname>
*docker run -d -p .....:.... imagename         : for port mapping

How to get into container /login to container
*docker attach <container id>  
--> will login to the container and gets attached to the main process.
--> if i run exit after attcahing to a container then it will stop /kill the container /process

2)we can log into a running container by creating new bash shell
*docker exec -it <container ID> /bin/bash
--> if we run exit it will stop the session bash it will not stop the main command of the container.

Q)how to safe exit from the container
<ctrl> +q+p
------------------------------------------------------------------------------------------------------------------------------------------------------

Docker images:-A Docker image is a file used to execute code in a Docker container. Docker images act as a set of instructions to build a Docker. 

1)official images: These are the images privided by founding companies of the tool/software. Ex: jenkins, ubuntu etc
2)base image: base image is the image used to create custom image 
3) Custome image:

-----------------------------------------------------------------------------------------------------------------------------------------------------
Docker file:
-> Dockerfile is the tool used to create custom images. sample way to build custom image and run container.
-----------------------------------------------------------------------------------------------------------------------------------------------------
mkdir docker-images
cd docker-images
mkdir apache 
cd apache 
vi Dockerfile

   FROM centos
   RUN yum -y install httpd
   CMD apachectl -DFOREGROUND 
-------------------------------
	$$ docker build -t apache_centos:v1 .
	$$ docker run -d -p 9090:80 <imagename:tag>
-d :detach mode
-p: port mapping

--------------------------------------------------------------------------------------------------------------------------------------------------------
To create a image from dockerfile
	$$ docker build -t <imagename>:<imagetag> <path of the Dockerfile>

To create a image from a existing container
	$$ docker commit <container id> <imagename>:<tagname>

to push the iamge to docker hub or  to the repository
 - each repository in docker hub refers to only one name
 - imagename should be also unique , can be same as repository name created in docker hub
 - tag name is user defined ,we can give any tag name
 -Docker hub preferred image name is <hub_username>/<reponame>:tag

1)We need to authenticate the repository login
syntax: docker login
2)push the image
syntax: $$ docker tag <local-image>:<tag> <hub_username>/<reponame>:<tag>   --->before pushing the image we need to tag the image properly
        $$ docker push <hub_username>/<repo_name>:<tag>		(login required)

	$$ docker pull deehub/tom22

*docker rm <containerid>  :to remove container
forcefully *docker rm -f <containerid>

------------------------------------------------------------------------------------------------------------------------------------------------------------
Dockerfile instructions
------------------------
FROM <image_name>:<tag>   ----->it used to specify the base image we want to start.
    -- if we dont specify the tag then automatically latest will be pulled
  -Ex: centos:7, python:2.7, Ubuntu:latest

RUN <command>   ----->to run the commands on top of base image
 -it can execute commands in exec form even there is no shell in the base image 
 -the default shell for execution can be changed by using SHELL <COMMAND>

COPY <src> <destination>
  -src is always calculated from the Dockerfile location
  -copies both files and directories from the host machine to the image

ADD <src> <destination>
   -copies both files and directories from the host machine to the image
   -of src is an URL , add will download , if its zipped it will be automatically extracted

CMD ["<executable>","<param1>","<param2>",.......]---> The main purpose of this CMD in Dockerfile is to define default execution point for the image or container
   - there can be only one CMD in docker file , if we specify multiple CMD then docker will consider last CMD specied in the dockerfile. 
   -CMD  can be overriden.

ENTRYPOINT ["<executable>","<param1>","<param2>",......]---> The main purpose of this Entrypoint in Dockerfile is to define default execution point for the image or container
   --ENTRYPOINT  cannot be overridden
   --if we use multiple ENTRYPOINT then last instruction will be considered.
   --during run time we can give --entrypoint so that it can be overwridden

what if we use both CMD and ENTRYPOINT in  same dockerfile then ENTRYPOINT has the highest priority , the command or paramas passed to the ENTRYPOINT will be considederd as executable
 commands specified through CMD will be input or prams to ENTRYPOINT 

WORKDIR ---->sets the path/working directory for RUN, CMD, ENTRYPOINT
 --use to define working directory of a docker container at any given time.

ENV <VAR_NAME><VALUE>
ENV  <VAR_NAME1><VALUE> <VAR_NAME2><VALUE> <VAR_NAME3><VALUE>---> It is used to create environment variables inside the image or container.

-> Dockerfile provides a dedicated variable type ENV to create an environment variable. We can access ENV values during the build, as 
	well as once the container runs.
-> We use docker run --env-file [path-toenv-file] to provide the environment variables to the container from a . env file
------------------------------------------------------------------------------------------------------------------------------------------------------------

Assignemt --->use following instructions and build a docker image and container 
  FROM ubuntu 
RUN apt-get update 
RUN apt-get install –y apache2 
RUN apt-get install –y apache2-utils 
RUN apt-get clean 
EXPOSE 80 
CMD [“apache2ctl”, “-D”, “FOREGROUND”]
------------------------------------------------------------------------------------------------------------------------------------------------------------

Sample Dockerfile for tomcat with .war
--------------------------------------
FROM ubuntu
RUN apt-get -y update && \
    apt-get -y install wget && \
    apt-get -y install openjdk-8-jdk && \
    apt-get -y install zip && \
    apt-get -y install unzip
RUN mkdir /tomcat
WORKDIR /tomcat
RUN wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.53/bin/apache-tomcat-9.0.53.zip && \
    unzip apache-tomcat-9.0.53.zip && \
    cd apache-tomcat-9.0.53/bin
COPY SimpleWebApplication.war /tomcat/apache-tomcat-9.0.53/webapps
RUN chown -R root:root /tomcat && \
    chmod -R +x apache-tomcat-9.0.53/bin && \
     chmod -R +x apache-tomcat-9.0.53/webapps
USER root
EXPOSE 8080
CMD /tomcat/apache-tomcat-9.0.53/bin/catalina.sh run
------------------------------------------------------------------------------------------------------------------------------------------------------------

** Docker has two options for containers to store files on the host machine, so that the files are persisted even after the container stops: volumes, and bind mounts.

-> Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Linux). 
Non-Docker processes should not modify this part of the filesystem. Volumes are the best way to persist data in Docker.

-> Bind mounts may be stored anywhere on the host system. 
They may even be important system files or directories. 
Non-Docker processes on the Docker host or a Docker container can modify them at any time.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Volumes Advantages:
-------------------
volumes are manged by docker itself. Default location for docker volumes is /var/lib/docker/volumes.
Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands or the Docker API.
Volumes work on both Linux and Windows containers.
Volumes can be more safely shared among multiple containers.
Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
New volumes can have their content pre-populated by a container.
Volumes on Docker Desktop have much higher performance than bind mounts from Mac and Windows hosts

Volumes commands 
#docker volume create <volume name>---> to create docker volumes
#docker volume ls -->list all the volumes
#docker volume rm <volume name>----> to delete docker volume
#docker volume prune   ---->delete unused volumes
#docker volume inspect <volume name> --->to display the details about volume
#docker run -v <volume name>:<continaer path>  --> attaching volume
--> $$ docker run -it -v vol1:/root/vol1 --name tomcat_ubuntu ubuntu: mount container to the volume, or attach container to volume
    $$ docker run -it -v vol1:/root/vol1 --name <imagename> ubuntu
------------------------------------------------------------------------------------------------------------------------------------------------------------

There are three types of volumes to consider:
1. Named volumes: have a specific source from outside the container, for example, awesome:/bar .
2. Anonymous volumes: have no specific source, therefore, when the container is deleted, you can instruct the Docker Engine 
                      daemon to remove them.
3. Host volumes: A host volume can be accessed from within a Docker container and is stored on the host, as per the name. 
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker networking: Docker networking allows you to attach a container to as many networks as you like.
-> Docker networking is primarily used to establish communication between Docker containers and the outside world via 
   the host machine where the Docker daemon is running.

1. Bridge Network:-This is a private network created by docker on the host when we install docker.
-> All the containers connected to the same bridge can communicate each other by default
-> All containers get an internal private Ip and these containers by default under the bridge network
-> Whenever we create a container by default without configuring any network it will be created under bridge by name docker0
-> Remove all unused network Use the $ docker network prune command to remove all unused networks. $ docker network prune
-> User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host.

#create a custom bridge network
	$$ docker network create --driver bridge <network_name>
#To create a container in the custom bridge
 	$$ docker run -dti --network <network_name><imagename:tag>

##custom bridge allows us to ping/they can communicate with container name and Ip

##To connect containers present in diffrent bridge
	$$ docker network connect <network_name> <container id>
        $$ docker network disconnect <network_name> <container id>       

2. HOST NETWORK:-
-> This driver removes the network isolation between the host machine and the docker. The containers are directly connected to host network.
   It depends upon us when to use this network as per requirement. 
   
	$$ docker run -d -it --network host <imagename:tag>
-> Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.

3. NONE NETWORK:-
-> Containers are not attcahed to any network on the host machine
-> This containers will not have any IP allocated
-> The services on this containers cannot be accessed by the outside world.
-> This option is used when a user wants to disable the networking access to a container.
-> In simple terms, None is called a loopback interface, which means it has no external network interfaces. 
Ex: perform operation (ansible vault) then we can asaign none network

4. Macvlan NETWORK:-
-> It simplifies the communication process between containers.
-> This network assigns a MAC address to the Docker container. With this Mac address, the Docker daemon routes the network traffic to a router.
-> Note: Docker Daemon is a server which interacts with the operating system and performs all kind of services.
-> It is suitable when a user wants to directly connect the container to the physical network rather than the Docker host.
-> Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.

5. Overlay network: create internal private netwrok, 
-> built on top of an existing network infrastructure. It allows for the creation of logical network connections between network devices that are not directly connected to each other.
-> This is typically achieved by encapsulating the original network packets in a new packet format and then transmitting them over the underlying network. 
-> Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other. 
-> You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. 
-> This strategy removes the need to do OS-level routing between these containers.
-> To create an overlay network for use with swarm services: 
	$$ docker network create -d overlay my-overlay
-> To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag:
	$$ docker network create -d overlay --attachable my-attachable-overlay
-> Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services.
-> This allows for the creation of virtual networks that span multiple physical networks, and can be used to provide advanced networking 
   features such as virtual private networks (VPNs), software-defined networking (SDN), and network function virtualization (NFV).

------------------------------------------------------------------------------------------------------------------------------------------------------------
EXPOSE: exposing a port is done in dockerfile 
	$$ EXPOSE <port_number>
-> allows container to communicate with each other on the exposed port 
-> the service in the container is not accessible from the outside world but can be accessible inside the containers 
-> This is applicable for inter container communication 

PUBLISH: 
-> allow containers to talk to each other and  to the outside world 
	$$ PUBLISH = EXPOSE + PUBLISH 
-> the service inside the container is accessible from anywhere 

to publish a single port 
	$$ docker run -p <hostport>:<container_port> .....

to publish all the available exposed port of the container 
	$$ docker run -P ..............

Publshing range of ports 
many to many - if we want to publish range ports then the range must match the number of ports between the docker published ports and host ports 
	$$ docker run -p 8081-8085:9091-9095
one to many : 
we can specify range of host ports where docker automatically publshes the container port to the one of the available port on host with in the range of host port 
	$$ docker run -p 8081-8085:8080
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker compose: V2.3.3
-> Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services.
   Then, with a single command, you create and start all the services from your configuration.

to Install docker compose follow
 https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-22-04
 
and create a sample docker compose project.

version--->mandatory-version of docker compose mandatory
services--->mandatory-services you want to bring up
volumes--->optional
network--->optional
------------------------------------------------------------------------------------------------------------------------------------------------------------

docker compose up -d :to start docker compose file
docker compose logs-->to check logs generated by the compose
docker compose pause--->If you want to pause the environment execution without changing the current state of your containers
               unpause
docker compose stop--->The stop command will terminate the container execution, but it won’t destroy any data associated with your containers
docker compose down--->If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command
------------------------------------------------------------------------------------------------------------------------------------------------------------

docker-compose.yml  for bringing sonarqube with postgress db
-----------------------------------------------------------------------------------------------------------------------------------------------------------

version: "3"

services:
  sonarqube:
    image: sonarqube:lts
    container_name: sonarqube
    ports:
      - 9000:9000
    networks:
      - sonarnet
    environment:
      - SONARQUBE_JDBC_URL=jdbc:postgresql://db:5432/sonar
      - SONARQUBE_JDBC_USERNAME=sonar
      - SONARQUBE_JDBC_PASSWORD=sonar
    volumes:
      - sonarqube_conf:/opt/sonarqube/conf
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_extensions:/opt/sonarqube/extensions
      - sonarqube_bundled-plugins:/opt/sonarqube/lib/bundled-plugins

  db:
    image: postgres:11.5
    container_name: postgres
    ports:
      - 5432:5432
    networks:
      - sonarnet
    environment:
      - POSTGRES_USER=sonar
      - POSTGRES_PASSWORD=sonar
    volumes:
      - postgresql:/var/lib/postgresql
      - postgresql_data:/var/lib/postgresql/data

networks:
  sonarnet:

volumes:
  sonarqube_conf:
  sonarqube_data:
  sonarqube_extensions:
  sonarqube_bundled-plugins:
  postgresql:
  postgresql_data:
-----------------------------------------------------------------------------------------------------------------------------------------------------------

Q. what is the command syntax for retrieving container logs?
-> The docker logs command instructs Docker to fetch the logs for a running container at the time of execution. 
   It only works with containers utilizing the JSON-file or journald logging driver.
-> The command syntax for retrieving container logs is:
	$$ sudo docker container logs [option] container_id

Q. How can I see all logs of a docker container?
-> First of all, to list all running containers, use the docker ps command. Then, with the docker logs command you 
   can list the logs for a particular container. Most of the time you'll end up tailing these logs in real time, 
   or checking the last few logs lines.
   	$$ docker ps -q | xargs -L 1 docker logs

-> Replace container_id with the ID number of the container you want to inspect. 
   To find the container ID, use the docker ps command to list running containers.
------------------------------------------------------------------------------------------------------------------------------------------------------------

*To copy a file from container to local machine.
sudo docker cp <Container ID>:<Path of file inside the container> <Path in the local machine>

*To copy a file from local machine to container
sudo docker cp <Path in the local machine> <Container ID>:<Path of file inside the container>
------------------------------------------------------------------------------------------------------------------------------------------------------------

What is a .dockerignore file?
Similar to a .gitignore file, a .Dockerignore files allows you to mention a list of files and/or directories which you might want to ignore while building the image. 
This would definitely reduce the size of the image and also help to speed up the docker build process. 
------------------------------------------------------------------------------------------------------------------------------------------------------------

To limit the maximum amount of memory usage for a container, add the --memory/-m option to the docker run command. 
Within the command, specify how much memory you want to dedicate to that specific container. 
The command should follow the syntax: 
	$$ sudo docker run -it --memory="[memory_limit]" [docker_image]

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How will you monitor Docker in production?
-> Docker provides tools like docker stats and docker events to monitor Docker in production.
   We can get reports on important statistics with these commands.
*Docker stats: When we call docker stats with a container id, we get the CPU, memory usage etc of a container. It is similar to top command in Linux.
*Docker events: Docker events are a command to see the stream of activities that are going on in Docker daemon.
-> Some of the common Docker events are: attach, commit, die, detach, rename, destroy etc.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How Multi-stage Build Works
-> With multi-stage builds, you use multiple FROM statements in your Dockerfile.
   Each FROM instruction can use a different base, and each of them begins a new stage of the build.
   We can selectively copy artifacts from one stage to another, leaving behind everything we don’t want in the final image.
-> In multi-stage build, we pick the tasks of building and running our applications into different stages. Here, we start with a 
   large image that includes all of the necessary dependencies needed to compile the binary executable of our application. 
   This can be termed as the builder stage.
-> We then took a lightweight image for our run stage which includes only what is needed to run a binary executable. 
   i.e just having jre in our final stage is sufficient to run our application. This can be termed as the production stage.

-> We can use multiple FROM commands combined with AS commands in our Dockerfile where the last FROM command will actually build the image. 
   All the FROM commands before that, will lead to the creation of intermediate images which are cached regularly.
-> The AS command when used with the FROM command allows us to provide a virtual name for our intermediate images.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How container achieve isolation
-> Docker containers achieve isolation by leveraging (barrow, use something to maximum advantage) Linux features like control groups 
   (commonly abbreviated as cgroups), secure computing mode (seccomp) filters, and kernel namespaces. 

Q. What are docker namespace and control groups?
-> When you start a container with docker run, behind the scenes Docker creates a set of namespaces and control groups for the container. 
   Namespaces provide the first and most straightforward form of isolation: processes running within a container cannot see, and even less affect, 
   processes running in another container, or in the host system.
-> Control group: A Control Group can be used to limit the number of resources that a particular process can use. 
   A control group can be used to limit the amount of memory that a process can use the amount of CPU, 
   the amount of hard drive input-output and the amount of network bandwidth as well.

------------------------------------------------------------------------------------------------------------------------------------------------------
My Docker Development Workflow: Code, Build, Push, Run

1. Write the code (and test it)
2. Build a container image
3. Push the image to the server
4. Restart the application, with the new image

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. What is monolithic appn and microservices and why are we moving from monolithic to microservices?
->  A monolithic application is built as a single unified unit while a microservices architecture is a collection of smaller, 
    independently deployable services.
-> Because you can detect bugs or performance issues in a gradual fashion.

Q. How do I set an environment variable in Docker?
-> Use the --build-arg option
	$$ docker build --build-arg [arg-variable]=[value]
   The output shows that Docker processed the ARG value and assigned it to ENV .

Q. How to take snapshot of container?
-> The command $$ docker commit takes a snapshot of your container. That snapshot is an image, which you can 
   put on a (private) repository to be able to pull it on another host. An option that does not use an image 
   (which you say you want to avoid) is indeed save and load.

Q. How many containers can be run per host?
-> Using this simple calculation, we can estimate that we can run about 1,000 containers on a single host with 10GB of available disk space.

Q. what is out of memory error in docker?
-> If a container is using an unexpected amount of either type of memory, it runs out of memory without affecting other 
   containers or the host machine. 
   Within this setting, if the kernel memory limit is lower than the user memory limit, 
   running out of kernel memory causes the container to experience an OOM error.
   However, it would be fine if the root user is used rather than the created user.
-> On Linux systems, if the kernel detects that there is not enough memory to perform some system task then it throws OOM 
   or out of memory error and starts killing the processes to free up some memory. 
-> Any process can be killed by kernel. That means it can kill Docker and Docker containers as well. Which can be a high risk if your
   application is running on  production and suddenly due to out of memory error occurred , your container was killed
   and your production application faces some down time. So it is very important to understand and control this memory issue by docker.
Q. What we can do? 
1. We need to perform some analysis to understand the application's memory requirement before deploying it to production.
2. Make sure you are using a host which has sufficient resources to run your containers and application.
3. Control your container's behaviour and restrict it's too much memory consumption.
   $$ sudo docker run -it --memory="[memory_limit]" [docker_image]
   
Q. Is it possible to edit the base image?
-> It is possible to edit the base image of a container, but it is generally not recommended as it can 
   lead to issues with compatibility and consistency.
-> A base image is the foundation of a container, it contains the operating system and any necessary libraries 
   and dependencies. When you create a container, you can add additional files and configurations on top of the base image.
-> If you want to make changes to the base image, you can do so by creating a new image from the existing 
   one using the "docker commit" command. However, this can lead to issues with compatibility and consistency.
-> A better approach is to use a versioned base image and update your container with the new version of the image. 
   This way you can ensure that the changes made to the base image have been thoroughly tested and validated by 
   the image maintainer.

Q. Deploying onto host. deploying onto container, which one u will prefer?
1. Deploying an application onto a host has the advantage of being simple and straightforward, as it does 
   not require any additional infrastructure or configuration. It also allows for more flexibility in terms 
   of resource allocation, as the application can use the resources of the host directly. 
   However, this approach can lead to issues with compatibility and consistency, as different applications 
   may have different dependencies and requirements.
2. Deploying an application onto a container, on the other hand, has the advantage of providing a consistent 
   and isolated environment for the application to run in. This makes it easier to manage the application's 
   dependencies and to ensure compatibility with other applications. Additionally, containerization can 
   provide better scalability, portability, and security. However, this approach can be more complex and 
   requires additional infrastructure, such as a container orchestration platform, in order to manage the 
   containerized applications.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Podman: An open-source tool that is similar to Docker in terms of functionality but does not require a daemon to run. 
        It is built to be more secure and is compatible with the Docker command-line interface.

CRI-O: A lightweight container runtime that is specifically designed to be used with Kubernetes. 
       It is built to integrate well with the Kubernetes API and is designed to be more secure and efficient than Docker.

containerd: An open-source container runtime that is designed to be lightweight and modular. 
            It is designed to be used as a component of other container orchestration platforms and 
            is used by projects such as Kubernetes and Docker itself.

rkt: A container runtime that is designed to be more secure and efficient than Docker. 
     It uses a different approach to container management and is designed to be more lightweight and less resource-intensive than Docker.

LXD: A container hypervisor that is similar to a traditional virtual machine hypervisor. 
     It uses Linux Containers (LXC) to provide isolated, lightweight environments that are similar to virtual machines.

Firecracker: A microVM runtime that is designed to be lightweight and secure. 
             It is built for running serverless workloads and is used by AWS for its Lambda and Fargate services.
------------------------------------------------------------------------------------------------------------------------------------------------------------

-> A lot of junior DevOps engineers get puzzled by the concept of CI/CD. 

CI stands for Continuous Integration. The sole purpose of CI is to give developers rapid feedback about their code quality with respect to the rest of the project. 
Once you push some changes to the git repository (or SVN or whatever you prefer) and you want to merge it to the main branch, you raise a PR (pull request). 
The PR triggers the CI pipeline which compiles and builds the code (if applicable) and performs some automated tests against it. Tests can be as simple or as complex 
as you want them to be (unit tests, code coverage, etc.) Basically, you’re examining whether your code “integrates” well with the application. If tests pass, 
the code can be merged into the main branch, usually with another team-member’s approval. Ideally, the CI pipeline should prepare an artifact and push it to an antifactory. 
The artifact can be as simple as a zip file containing the code files in compressed form or as complex as it needs to be (EXE file, JAR archive, Ruby Gem, Helm package, etc.) 
The artifactory is a special type of storage that allows versioning. For example, AWS S3, ECR, Nexus and others.

Continuous Delivery (CD) makes sure that code not only integrates well with the application, but it is also deployable to an environment 
that is as close as possible to production. 
If the application follows the microservices architecture, the CD pipeline performs tests against the entire system (all the APIs) like end-to-end tests and User Acceptance Tests (UAT). 
The important thing to note here is that CD pulls the artifact from the artifactory and “delivers” it to one or more environments. The artifact remains the same in all stages of CD since it is the one where all tests were done against. When it comes to delivering the artifact to production, it must be done manually due to the obvious criticality. 
Someone selects which artifact version can be released to production and clicks a button that triggers the same CD process but against the live environment. CD involves working with deployment which ranges from just uploading files through FTP or rsync to using configuration management tools like Ansible or AWS SSM. 
If the environment is containerized, then we’d be working with kubectl or helm and kustomize. Environments can also be created and destroyed on the fly using IaC tools like Terraform,  and Packer.
In more advanced (and mature) scenarios, deployment to production is also automated, which is referred to as continuous deployment. However, it requires a whole suite of thorough tests to guarantee code and application quality in production. QA here involves functional testing, performance, and stress testing among several others. When it comes to deployment, CD should also feature the ability of rolling-back failed deployments. Some of those techniques are achieved via blue/green deployments and/or feature gates.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Q. What is depends_on in docker
-> depends_on is a Docker Compose keyword to set the order in which services must start and stop.

Q. How to Import docker container in Kubernetes?
-> you can use the kubectl command-line tool to create a new Kubernetes pod and run the Docker container inside it.
1. Pull the Docker container image: first need to download the Docker container image from a registry. You can do this using the docker pull command.
2. Create a Kubernetes pod: Once you have the Docker container image, you can use the kubectl run command to create a new Kubernetes pod 
   that will run the Docker container. The kubectl run command creates a deployment and a pod, so you don't need to create a deployment and a pod separately.
3. Specify the Docker container image: When you create the Kubernetes pod, you need to specify the Docker container image that you 
   want to run. You can do this by using the --image option with the kubectl run command.
4. Expose the pod: By default, pods in Kubernetes are only accessible from within the cluster. To make the pod accessible from outside the cluster, 
   you need to expose it using a Kubernetes service. You can use the kubectl expose command to expose the pod as a service.
5. Verify the pod is running: You can use the kubectl get pods command to verify that the pod is running. You should see the pod in the list of running pods.

Q. What are the containers? which are running?
-> Containers are a form of lightweight, stand-alone, and executable software packages that include everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and config files.
   $$ docker ps : list the running containers
------------------------------------------------------------------------------------------------------------------------------------------------------------

Write a Dockerfile in which you have to pull the image from the registry and set hardcoded environment variable and then
copy some dependices file to filesystem of container and then create a directory to somepath
and copy projectfile to newly created directory and give read,write and execute permission to newly directory
and copy another one docker script and make that script executable.
Dockerfile:
==========
# Pull the base image from the registry
FROM registry-image:latest

# Set an environment variable
ENV MY_VAR=my_value

# Copy dependencies file to the container filesystem
COPY dependencies.txt /app/dependencies.txt

# Create a directory and copy project files to it
RUN mkdir -p /app/project
COPY projectfile /app/project/projectfile

# Give read, write, and execute permissions to the project directory
RUN chmod 777 /app/project

# Copy the Docker script to the container filesystem and make it executable
COPY my_script.sh /app/my_script.sh
RUN chmod +x /app/my_script.sh

# Set the working directory to the project directory
WORKDIR /app/project

# Start the container by running the script
CMD ["/app/my_script.sh"]
