Docker Commands:

1. $$ docker –version    : This command is used to get the currently installed version of docker
2. $$ docker ps : list the running containers
3. $$ docker ps -a : show all the running and exited containers
4. $$ docker container ls -s : to view approximate size of a running container
3. $$ docker pull <image name>	:This command is used to pull images from the docker repository(hub.docker.com)
4. docker build: To create a image from dockerfile
   $$ docker build -t <imagename>:<tag> <path of the Dockerfile>
10. docker commit: To create a image from a existing container
   $$ docker commit <container id> <imagename>:<tag>
5. docker run: This command is used to run a container from an image
   $$ docker run -it -d <image name>:tag
6. docker attach: get into container /login to container
   $$ docker attach <container id>
7. docker exec: used to access/login the running container (in order to run in the session of bin bash )
   $$ docker exec -it <container ID> /bin/bash
8. docker stop: stops a running container
   $$ docker stop <container id>
9. docker kill: This command kills the container by stopping its execution immediately
   $$ docker kill <container id>
10.docker rm: to remove/delete a stopped container
   $$ docker rm <containerid>  (forcefully *docker rm -f <containerid>)
   $$ docker rm $(docker stop <contaner id>)

10.docker login: login to the docker hub repository
   $$ docker tag <local-image>:<tag> <hub_username>/<reponame>:<tag>   --->before pushing the image we need to tag the image properly
   
   $$ docker build -t <imagename>:<tag> <path of the Dockerfile>
   $$ docker push <hub_username>/<repo_name>:<tag>		(login required)

 $$ docker push $DOCKER_REGISTRY_USERNAME/simple-python-flask-app:latest		(login required)

-> docker pull centos/alpine/wordpress/archlinux
-> pulling images for centos (installing centos) from docker image registry(called docker hub), to create and run a container
* $$ docker stop <containerID>
* $$ docker start <containerID>
* $$ docker stats <containerID> 
* $$ docker inspect <containerID>: 
* docker info   : it gives the information of the docker, like no of containers, kernel version, kernal name
* flags -p: assign port target
        -d: detached mode
        -v: attach volume
        -q: quit
        -f: filter 

11.$$ docker images: lists all the locally stored docker images
12.docker rmi: to delete an image from local storage
   $$ docker rmi <image-id>
   $$ docker image rm <imagename>:tag  Ex:nginx:alpine

13.$$ docker volume create <volume name>---> to create docker volumes
14.$$ docker volume ls --->list all the volumes
15.$$ docker volume rm <volume name> ----> to delete docker volume
16.$$ docker volume prune   ----> delete unused volumes
17.$$ docker volume inspect <volume name> ---> to display the details about volume
18.$$ docker run -v <volume name>:<continaer path>  --> attaching volume
   $$ docker run -it -v vol1:/root/vol1 --name <image name> ubuntu   : mount container to the volume, or attach container to volume

19.$$ docker network create --driver bridge <network_name>   --->create a custom bridge network
20.$$ docker run -dti --network <network_name><imagename:tag>  --->create a container in the custom bridge
21.Connect containers present in diff bridge
   $$ docker network connect <network_name> <container id>
   $$ docker network disconnect <network_name> <container id>
22.$$ docker run -d -it --network host <imagename:tag>   -->running a host network
23.To create an overlay network for use with swarm services: 
   $$ docker network create -d overlay my-overlay
   -> To create an overlay network which can be used by swarm services or standalone containers to communicate with other standalone containers running on other Docker daemons, add the --attachable flag:
   $$ docker network create -d overlay --attachable my-attachable-overlay

24.to publish a single port 
   $$ docker run -p <hostport>:<container_port> .....
   to publish all the available exposed port of the container 
   $$ docker run -P ..............
25.container logs: for retrieving container logs
  $$ sudo docker container logs [option] container_id
26.container logs: all logs of a docker container
  $$ docker ps -q | xargs -L 1 docker logs
27.docker memory: To limit the maximum amount of memory usage for a container, add the --memory/-m
  $$ sudo docker run -it --memory=" [memory_limit]" [docker_image]

28. Docker compose commands
docker compose up -d :to start docker compose file
docker compose logs-->to check logs generated by the compose
docker compose pause--->If you want to pause the environment execution without changing the current state of your containers
               unpause
docker compose stop--->The stop command will terminate the container execution, but it won’t destroy any data associated with your containers
docker compose down--->If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command

-------------------------------------------------------------------------------------------------------------------------------
Q. daemon login and container login
-> Daemon Login: In computing, a daemon is a background process that runs on a computer and performs specific tasks. 
-> A Daemon Login is a type of login in which a user is granted access to the computer as a daemon process, allowing them to run specific tasks in the background. 
-> This type of login is typically used by system administrators to perform administrative tasks, such as starting and stopping services or running scripts.

-> Container Login: A Container Login is the process of logging into a container to access its environment and run commands or make changes. This is typically done by using a command line interface (CLI) such as Docker or Kubernetes. 

Q. restart policies for docker container with example?
-> It tells Docker how to handle restart a container that has stopped.
    Here are the available restart policies:
1. No: Never attempt to restart the container. (
	$$ docker run --name my_container --restart=no my_image
2. On-failure: Restart the container only if it stops due to an error, and add a maximum restart count.
	$$ docker run --name my_container --restart=on-failure:5 my_image
3. Always: Always restart the container regardless of the exit status.
	$$ docker run --name my_container --restart=always my_image
4. Unless-stopped: Restart the container always, unless it is explicitly stopped.
	$$ docker run --name my_container --restart=unless-stopped my_image

Q. What is the difference containerd and runC?
-> containerd is called a high-level container runtime. For some actions, it makes use of yet another runtime, called a low-level container runtime. This low-level runtime is called runc

Q. What is docker system prune?
-> docker system prune is not safe to be used in production. It may clean up and reclaim space but there's a possibility that one 
   or more containers will die and need to be restarted manually. In production apps, you want the images to fail gracefully and 
   restart automatically.

Q. What are orphaned containers?
-> Orphaned containers are basically containers you used previously, but you deleted the dependencies to them so you can't use them anymore, but they are still present on your computer.
  $$ docker-compose up -d --remove-orphans

Q. Docker inspect: provides detailed information on constructs controlled by Docker, render results in a JSON array
 $ docker volume inspect

Q. Diff between running and creating the container?
-> Start will start any stopped containers. This includes freshly created containers. 
   Run is a combination of create and start. It creates the container and starts it.
-> ARG will be available only while building the image. while ENV variables are available while building and also while running the container.

Q. Difference between Docker kill and Docker stop:
-> To terminate a container, Docker provides the docker stop and docker kill commands. 
   Both the docker kill and docker stop commands look similar, but their internal execution is different. 
   The docker stop commands issue the SIGTERM signal to the main process running, whereas the docker kill commands sends the SIGKILL signal to the process.
-> ‘docker stop’ gives the container time to shutdown gracefully, in situations when it is taking too much time for getting the container to stop, one can opt to kill it

Q. The three primary differences between the Dockerfile and docker-compose are:
-> The Dockerfile is used to build images while the docker-compose.yaml file is used to run images.
-> The Dockerfile uses the docker build command, while the docker-compose.yaml file uses the docker-compose up command.
-> A docker-compose.yaml file can reference a Dockerfile, but a Dockerfile can’t reference a docker-compose file.

Q. how to login to docker repository?
-> You can log into any public or private repository for which you have credentials. 
   When you log in, the command stores credentials in $HOME/. docker/config. json on Linux or %USERPROFILE%/.

Q. Difference between docker image and layer?
-> A Docker image is a package that contains all the files and dependencies needed to run a specific application or service. 
   It is built from one or more layers, which are stacked on top of each other to create the final image.
-> A Docker layer, also known as an image layer, is a set of changes to the file system that make up a specific version of an image. 
   Each layer is represented by a unique identifier, called a "digest," which is generated by a hash of the contents of the layer.
-> Image is the final product, while a layer is a building block used to create the image.

Q. docker layer
-> the fundamental building blocks for creating, deploying, and scaling systems.
-> Docker stores all caches in /var/lib/docker/<driver> , where <driver> is the storage driver overlay2 again
---------------------------------------------------------------------------------------------------------------

*Assignment- Dangling images and how to delete them.
Ans: Simply an unused image that’s got no name and tag. You can easily spot dangling images when you run the $$ docker images command because they show up as <none>:<none>.

*docker system prune :Remove all dangling images. If -a is specified, will also remove all images not referenced by any container.
--all , -a		Remove all unused images, not just dangling ones
--filter		Provide filter values (e.g. 'until=<timestamp>')
--force , -f		Do not prompt for confirmation

List dangling images:
$ sudo docker images -f dangling=true

*How Are Dangling Images Created?
-> Dangling images are usually created when an existing image gets superseded by a new build.
*Can You Use a Dangling Image?
-> Dangling images function like any other image. The only difference is the missing tag. 
-> You can start a container from a dangling image by directly referencing the image’s ID.

*Cleaning Up Dangling Images
-> You can delete a single dangling image using the docker rmi command, just like any other image. 
   Because the image won’t be tagged, you’ll need to identify it by its ID.
-> Docker image prune :Remove all dangling images.
-----------------------------------------------------------------------------------------------------------------------------------------------------

Why Docker
----------------
Suppose there are four developers in a team working on a single project.
Meanwhile, one is having a Windows system, the second is owning a Linux system, and the third & fourth ones are working with macOS. Now, as you see, they are using the distinct environments for creating a single application or software they will be required to carry on the things in accordance with their respective machines such as the installation of different libraries & files for their system, etc. And such situations, especially on an organizational or larger level, often cause numerous conflicts and problems throughout the entire software development life cycle. However, the containerization tools such as Docker eliminates this problem.

--------------------------------------------------------------------------------------------------------------------------------------------------------
** Tomcat: Tomcat is widely used by web developers when working on web application development. 
-> From a high-level perspective, apache tomcat is responsible to provide a run-time environment for the servlets. 
-> It provides an environment in which one could run their java code.

-> Tomcat: Born out of the Apache Jakarta Project, Tomcat is an application server designed to execute Java servlets and render web pages that 
  use Java Server page coding. Accessible as either a binary or a source code version, Tomcat’s been used to power a wide range of applications 
  and websites across the Internet.

** Catalina.sh:
-> It is the script that is actually responsible for starting Tomcat; the "startup" script simply runs "catalina" with the argument "start" 
("catalina" also can be used with the "stop" parameter to shut down Tomcat).

** Start up.sh: Located in the /base/scripts directory, the startup script (startup.sh) is run by the system boot process. 
** Near the end of startup.sh is a list of services that will be run upon startup.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker (Docker --Version: 20.10.12): 
-> Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. 
-> Containers allow a developer to package up an application with all of the parts it needs, such as 
   libraries and other dependencies, and ship it all out as one package.

** Pre-requisite for docker: Ubuntu server setup with non sudo root user & a firewall, docker hub account
------------------------------------------------------------------------------------------------------------------------------------------------------------

Containerization and virtualization:
-> Virtualization aims to run multiple OS instances on a single server, whereas containerization runs a single OS instance, with multiple user spaces 
   to isolate processes from one another.
-> This means containerization makes sense for one AWS cloud user that plans to run multiple processes simultaneously.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Diffrence between VM and container.
The key differentiator between containers and virtual machines is that virtual machines virtualize an entire machine down to the hardware layers
and containers only virtualize software layers above the operating system level.
	          		Docker	                                                      Virtual Machines (VMs)
Boot-Time		Boots in a few seconds.	                                	It takes a few minutes for VMs to boot.
Runs on	        	Dockers make use of the execution engine/docker engine.	       	VMs make use of the hypervisor.
Memory Efficiency    	No space is needed to virtualize, hence less memory. 		Requires entire OS to be loaded before starting the surface, so less efficient. 
Deployment		Deploying is easy as only a single image, 			Deployment is comparatively lengthy as separate instances 
               		containerized can be used across all platforms.   		are responsible for execution.		

-> A hypervisor, also known as a virtual machine monitor or VMM, is software that creates and runs virtual machines (VMs). 
-> A hypervisor allows one host computer to support multiple guest VMs by virtually sharing its resources, such as memory 
  and processing.
Types:
1. Type 1 (Bare metal) : acts like a lightweight operating system and runs directly on the host's hardware
2. Type 2 (Hosted) :  runs as a software layer on an operating system, like other computer programs.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker architecture:
https://docs.docker.com/get-started/overview/

Docker uses a client-server architecture. 
*The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. 
*The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. 
*The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. 
*Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.

The Docker client: 
-> The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as 
   docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. 
   The Docker client can communicate with more than one daemon.

The Docker daemon: 
-> The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. 
   A daemon can also communicate with other daemons to manage Docker services.

Docker Desktop:
-> Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and 
   share containerized applications and microservices. 
-> Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop.

Docker registries:
-> A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for 
   images on Docker Hub by default. You can even run your own private registry.
-> When you use the docker pull or docker run commands, the required images are pulled from your configured registry. 
   When you use the docker push command, your image is pushed to your configured registry. (To push we login to dockerhub)
-> Private registry: Amazon ECS(Elastic container service), Docker enterprise, Azure Kuberntes service, Google Kubernetes Engine (GKE)
-> Other registries: Google Container Registry, Amazon Elastic Container Registry, Azure Container Registry, GitLab Container Registry, 
   JFrog Container Registry, Quay, Harbor.

Docker Engine overview:
-> Docker Engine is an open source containerization technology for building and containerizing your applications. 
   Docker Engine acts as a client-server application with:
* A server with a long-running daemon process dockerd.
* APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
* A command line interface (CLI) client docker.
-> The CLI uses Docker APIs to control or interact with the Docker daemon through scripting or direct CLI commands. 
   Many other Docker applications use the underlying API and CLI. 
   The daemon creates and manage Docker objects, such as images, containers, networks, and volumes.

-> Docker container: A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly 
   and reliably from one computing environment to another. 
-> A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: 
   code, runtime, system tools, system libraries and settings.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker Container Lifecycle Management or stages of container:
1. Created state : docker create --name <name-of-container> <docker-image-name>
2. Running state : docker run <container-id or container-name> 
3. Paused state/unpaused state: docker pause container <container-id or container-name> & docker unpause <container-id or container-name>
4. Stopped state: docker stop <container-id or container-name>
5. Killed/Deleted state : docker kill <container-id or container-name>

"docker pause":
-- Suspends all container processes (freezes them)
-- Graceful Shutdown: No (just pauses processes)
-- Container remains running (paused, not terminated)
-- Resources (e.g., memory, CPU) are still allocated
-- Temporarily freeze processes (e.g., maintenance)

"docker stop":
-- Gracefully stops container processes (via SIGTERM)
-- Graceful Shutdown: Yes (tries to gracefully stop processes)
-- Container is completely stopped (terminated)
-- Resources are released when the container stops
-- Gracefully stop and release resources (e.g., service shutdown)

"docker kill":
-- Forcefully stops container processes (via SIGKILL)
-- Graceful Shutdown: No (forcefully kills processes)
-- Container is completely stopped (terminated)
-- Resources are released immediately after kill
-- Emergency force stop when the container is unresponsive

---------------------------------------------------------------------------------------------------------------------------------------------------
Install docker
--------------------------------------------
https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-22-04
to run without sudo user use-

sudo usermod -aG docker ubuntu   ----->adding ubuntu into docker group
sudo systemctl enable docker   ----->starts docker whenever machine starts

docker run -it -p 8080:8080 jenkins/jenkins:lts    ------>docker command to run jenkins
it: interactive terminal

*Assignment install sonarqube using docker--->https://techexpert.tips/sonarqube/sonarqube-docker-installation/

---------------------------------------------------------------------------------------------------------------------------------------------------
Docker commands:
** docker stats [OPTIONS] [CONTAINER...]  -->Display a live stream of container(s) resource usage statistics.
** docker info --> displays system wide information regarding the Docker installation (kernel version, number of containers and images)
** docker ps-->list the running containers
** docker ps -q   :(--quiet only the container IDs)
** docker ps -a -->list all the containers

** docker container ls---> to list the running containers
** docker	image ls
** docker	volume ls
** docker	network ls

** docker ps -f status=exited --> list only exited containers or stopped containers   (-f // --filter)
** docker ps -f status=exited | xargs -l {}docker rm                    alternate command docker rm $(docker ps -q -f status=exited)
** docker start $(docker ps -a -q --filter "status=exited")



** docker save <container name>  : to save file to local desktop
** How to Create a .tar File for Loading (Using docker save):
-> To use docker load, you need a .tar file created with the docker save command. For example:
1. Save an image to a file:
$$ docker save -o my_image.tar my_image:latest
2. Transfer or share the .tar file, and it can then be loaded using docker load.


** docker load < my_image.tar:
-> to load a Docker image from a tar archive (usually created using the docker save command)
-> This allows you to import Docker images that have been exported to a .tar file, typically for sharing or transferring images between environments where a direct Docker registry (e.g., Docker Hub) is not accessible.

1. Loading an Image from a .tar File:
$$ docker load --input my_image.tar : 
-- Output: Loaded image: my_image:latest

2. Loading Multiple Images from a .tar File:
$$ docker load < multiple_images.tar
Ouput: Loaded image: image_one:tag1
Loaded image: image_two:tag2

3. Suppress Output During Loading:
docker load --quiet < my_image.tar

---------------------------------------------------------------------------------------------------------------------------------------------------
Best way To delete all the running containers
-> stop all running containers
** docker stop $(docker ps -q)
-> then delete the stopped containers
** docker rm $(docker stop $(docker ps -q))

** docker rm $(docker stop <contaner id>)

Automatically delete the container when it stopped
*docker run --rm 

To create a container in background /in detached mode (If we don't run in detach mode we can run commands)
** docker run -d <imagename>:<tagname>
** docker run -d -p 8080:80 imagename         : for port mapping

1. How to get into container / login to container
** docker attach <container id>  
--> will login to the container and gets attached to the main process.
--> if i run exit after attaching to a container then it will stop /kill the container /process

2. we can log into a running container by creating new bash shell
** docker exec -it <container ID> /bin/bash
--> if we run exit it will stop the session bash it will not stop the main command of the container.
-> Below folders will be present in a container when you do exec:
bin  boot  dev  etc  home  lib  lib.usr-is-merged  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var

Q)how to safe exit from the container
<ctrl> +q+p
------------------------------------------------------------------------------------------------------------------------------------------------------

Docker images:
---------------
-> In Docker, an image is a lightweight, standalone, and executable package that contains everything needed to run a piece of software, including the application code, runtime, libraries, dependencies, and settings. 
-> Docker images are immutable and serve as the blueprint for creating Docker containers.

-> A Docker image is a prebuilt snapshot of a container’s file system and application state.
-> It is read-only and cannot be modified.
-> When you run a Docker image, it creates a container, which is a running instance of the image.

1. Official images: These are the images provided by founding companies of the tool/software. Ex: jenkins, ubuntu etc
-> Available on Docker Hub (https://hub.docker.com/)
-> Maintained and published by Docker or verified publishers.
-> These images are well-maintained, secure, and optimized.
-> Examples: nginx, mysql, node, python.

2. Base image: base image is the image used to create custom image 
-> These are foundational images that serve as the starting point for creating other images.
-> Base images do not have a parent image; they represent the lowest layer in the image hierarchy.
-> Example: ubuntu, alpine, debian, centos.

3. Custom image:
-> Created by users for their specific applications or requirements.
-> These images are built using a Dockerfile to specify how the image should be constructed.
-> Example: A custom web server image with preconfigured settings.

-----------------------------------------------------------------------------------------------------------------------------------------------------
Docker file:
-> A Dockerfile is a text file that contains instructions for building a Docker image. 
-> It acts as a blueprint, specifying all the steps, configurations, and dependencies required to assemble an image. 
-> Sample way to build custom image and run container.

-----------------------------------------------------------------------------------------------------------------------------------------------------
mkdir docker-images
cd docker-images
mkdir apache 
cd apache 
vi Dockerfile

   FROM centos
   RUN yum -y install httpd
   CMD apachectl -DFOREGROUND 
-------------------------------
$$ docker build -t apache_centos:v1 .
$$ docker run -d -p 9090:80 apache_centos:v1	
-d :detach mode
-p: port mapping

$$ docker build -t <imagename>:<imagetag> <path of the Dockerfile>
$$ docker run -d -p 9090:80 <imagename:tag>   => Running a container
$$ docker tag <local-image>:<tag> <hub_username>/<reponame>:<tag>   --->before pushing the image we need to tag the image properly
$$ docker push <hub_username>/<repo_name>:<tag>		(login required)

$$ docker pull <hub_username>/<repo_name>

$$ docker commit <container id> <imagename>:<tagname>   => To create a image from dockerfile/image
$$ docker rm <containerid>  => to remove container
$$ docker rm -f <containerid>   => forcefully

--------------------------------------------------------------------------------------------------------------------------------------------------------

** Docker Hub: To push the image to docker hub or  to the repository
 - each repository in docker hub refers to only one name
 - imagename should be also unique , can be same as repository name created in docker hub
 - tag name is user defined ,we can give any tag name
 -Docker hub preferred image name is <hub_username>/<reponame>:tag

1)We need to authenticate the repository login
syntax: docker login
2)push the image
syntax: $$ docker tag <local-image>:<tag> <hub_username>/<reponame>:<tag>   --->before pushing the image we need to tag the image properly
        $$ docker push <hub_username>/<repo_name>:<tag>		(login required)

------------------------------------------------------------------------------------------------------------------------------------------------------------
Dockerfile instructions
------------------------
FROM <image_name>:<tag>   ----->it used to specify the base image we want to start.
    -- if we dont specify the tag then automatically latest will be pulled
  -Ex: centos:7, python:2.7, Ubuntu:latest

RUN <command>   ----->to run the commands on top of base image
 -it can execute commands in exec form even there is no shell in the base image 
 -the default shell for execution can be changed by using SHELL <COMMAND>

COPY <src> <destination>
  -src is always calculated from the Dockerfile location
  -copies both files and directories from the host machine to the image

ADD <src> <destination>
   -copies both files and directories from the host machine to the image
   -of src is an URL , add will download , if its zipped it will be automatically extracted

CMD ["<executable>","<param1>","<param2>",.......]---> It defines default execution point for the image or container
   - there can be only one CMD in docker file , if we specify multiple CMD then docker will consider last CMD specified in the dockerfile. 
   -CMD can be overridden.

ENTRYPOINT ["<executable>","<param1>","<param2>",......]---> Define default execution point for the image or container
   --ENTRYPOINT  cannot be overridden
   --if we use multiple ENTRYPOINT then last instruction will be considered.
   --during run time we can give --entrypoint so that it can be overridden

--> what if we use both CMD and ENTRYPOINT in  same dockerfile then ENTRYPOINT has the highest priority, the command or parameters passed to the ENTRYPOINT will be considered as executable commands specified through CMD will be input or prams to ENTRYPOINT.

WORKDIR ---->sets the path/working directory for RUN, CMD, ENTRYPOINT
 --use to define working directory of a docker container at any given time.

ENV <VAR_NAME><VALUE>
ENV  <VAR_NAME1><VALUE> <VAR_NAME2><VALUE> <VAR_NAME3><VALUE>---> It is used to create environment variables inside the image or container.

EXPOSE 5000		# Expose the port the Flask application will be listening on

-> Dockerfile provides a dedicated variable type ENV to create an environment variable. We can access ENV values during the build, as 
	well as once the container runs.
-> We use "docker run --env-file [path-toenv-file]" to provide the environment variables to the container from a . env file
------------------------------------------------------------------------------------------------------------------------------------------------------------

Assignemt --->use following instructions and build a docker image and container 
FROM ubuntu 
RUN apt-get update 
RUN apt-get install –y apache2 
RUN apt-get install –y apache2-utils 
RUN apt-get clean 
EXPOSE 80 
CMD [“apache2ctl”, “-D”, “FOREGROUND”]

# Base image
FROM python:3.8
# Set the working directory inside the container
WORKDIR /app
# Copy the requirements file
COPY requirements.txt .
# Install the project dependencies
RUN pip install -r requirements.txt
# Copy the application code into the container
COPY . .
# Expose the port the Flask application will be listening on
EXPOSE 5000
# Set environment variables, if necessary
# ENV MY_ENV_VAR=value
# Run the Flask application
CMD ["python", "app.py"]
------------------------------------------------------------------------------------------------------------------------------------------------------------

Sample Dockerfile for tomcat with .war
--------------------------------------
FROM ubuntu
RUN apt-get -y update && \
    apt-get -y install wget && \
    apt-get -y install openjdk-8-jdk && \
    apt-get -y install zip && \
    apt-get -y install unzip
RUN mkdir /tomcat
WORKDIR /tomcat
RUN wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.53/bin/apache-tomcat-9.0.53.zip && \
    unzip apache-tomcat-9.0.53.zip && \
    cd apache-tomcat-9.0.53/bin
COPY SimpleWebApplication.war /tomcat/apache-tomcat-9.0.53/webapps
RUN chown -R root:root /tomcat && \
    chmod -R +x apache-tomcat-9.0.53/bin && \
    chmod -R +x apache-tomcat-9.0.53/webapps
USER root
EXPOSE 8080
CMD /tomcat/apache-tomcat-9.0.53/bin/catalina.sh run
------------------------------------------------------------------------------------------------------------------------------------------------------------

** Docker has two options for containers to store files on the host machine, so that the files are persisted even after the container stops: Volumes, and Bind mounts.

Volumes:
--------
-> Managed by Docker, stored in host filesystem /var/lib/docker/volumes/
-> Managed entirely by Docker	
-> Data persists even if the container is removed
-> Suitable for persistent application data (databases, logs, etc.)
-> Docker manages permissions automatically
-> Slightly slower than bind mounts due to Docker's abstraction
-> Non-Docker processes should not modify this part of the filesystem. 
-> Volumes are the best way to persist data in Docker.

Bind Mounts:
-----------
-> Located at a specified path on the host filesystem
-> Controlled by the host system and user
-> Data is tied to the host, may be affected by host changes
-> Suitable for local development or directly accessing host files
-> File permissions are controlled by the host
-> Typically faster, as it's a direct link to the host filesystem
-> They may even be important system files or directories. 
-> Non-Docker processes on the Docker host or a Docker container can modify them at any time.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Volumes Advantages:
-------------------
-> volumes are managed by docker itself. Default location for docker volumes is /var/lib/docker/volumes.
-> Volumes are easier to back up or migrate than bind mounts.
-> You can manage volumes using Docker CLI commands or the Docker API.
-> Volumes work on both Linux and Windows containers.
-> Volumes can be more safely shared among multiple containers.
-> Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
-> New volumes can have their content pre-populated by a container.
-> Volumes on Docker Desktop have much higher performance than bind mounts from Mac and Windows hosts

Volumes commands 
$$ docker volume create <volume name>---> to create docker volumes
$$ docker volume ls -->list all the volumes
$$ docker volume rm <volume name>----> to delete docker volume
$$ docker volume prune   ---->delete unused volumes
$$ docker volume inspect <volume name> --->to display the details about volume
$$ docker run -v <volume name>:<continaer path>  --> attaching volume
--> $$ docker run -it -v vol1:/root/vol1 --name tomcat_ubuntu ubuntu: mount container to the volume, or attach container to volume
    $$ docker run -it -v vol1:/root/vol1 --name <imagename> ubuntu
------------------------------------------------------------------------------------------------------------------------------------------------------------

There are three types of volumes to consider:
1. Named volumes: 
-> Named volumes are managed by Docker, and they are stored in a specific directory on the host system (usually /var/lib/docker/volumes/). Docker abstracts the location of these volumes, which makes them easier to use for data persistence and portability.
-> They have a specific source from outside the container, for example, awesome:/bar .
$$ docker run -d -v my_named_volume:/data --name my_container my_image


2. Anonymous volumes: 
-> Anonymous volumes are similar to named volumes, but they are created without specifying a name. Docker generates a unique name for them, which makes them useful for temporary data storage.
-> They have no specific source, therefore, when the container is deleted, you can instruct the Docker Engine daemon to remove them.
$$ docker run -d -v /data --name my_container my_image

3. Host volumes (Bind Mounts): 
-> A host volume can be accessed from within a Docker container and is stored on the host, as per the name. 
-> Host volumes, also known as bind mounts, are a way to mount a directory or file from the host system into the container. 
-> Unlike named or anonymous volumes, bind mounts map a specific location on the host filesystem to a location inside the container.
$$ docker run -d -v /host/path:/container/path --name my_container my_image

------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker networking: 
-----------------
-> Docker networking allows you to attach a container to as many networks as you like.
-> Docker networking is primarily used to establish communication between Docker containers and the outside world via 
   the host machine where the Docker daemon is running.

1. Bridge Network:-This is a private network created by docker on the host when we install docker.
-> All the containers connected to the same bridge can communicate each other by default.
-> All containers get an internal private Ip and these containers by default under the bridge network.
-> Whenever we create a container by default without configuring any network it will be created under bridge by name docker0
-> Remove all unused network Use the "$$ docker network prune" command to remove all unused networks. 
"$$ docker network prune"
-> User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host.

-> Default Docker CIDR Range: Bridge network: 172.17.0.0/16
-> This means Docker containers on the default bridge network will be assigned IP addresses in the range 172.17.0.1 to 172.17.255.254

## create a custom bridge network
	$$ docker network create --driver bridge <network_name>
## To create a container in the custom bridge
 	$$ docker run -dti --network <network_name><imagename:tag>

## custom bridge allows us to ping/they can communicate with container name and Ip

## To connect containers present in different bridge
	$$ docker network connect <network_name> <container id>
        $$ docker network disconnect <network_name> <container id>       

2. HOST NETWORK:-
-> This driver removes the network isolation between the host machine and the docker. The containers are directly connected to host network.
   It depends upon us when to use this network as per requirement. 
   
	$$ docker run -d -it --network host <imagename:tag>
-> Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.

3. NONE NETWORK:-
-> Containers are not attached to any network on the host machine
-> This containers will not have any IP allocated
-> The services on this containers cannot be accessed by the outside world.
-> This option is used when a user wants to disable the networking access to a container.
-> In simple terms, None is called a loopback interface, which means it has no external network interfaces. 
Ex: perform operation (ansible vault) then we can assign none network

4. Macvlan NETWORK:-
-> It simplifies the communication process between containers.
-> This network assigns a MAC address to the Docker container. With this Mac address, the Docker daemon routes the network traffic to a router.
-> It is suitable when a user wants to directly connect the container to the physical network rather than the Docker host.
-> Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.

5. Overlay Network: Create Internal Private Network
-> An Overlay network in Docker is a type of network that allows containers on different Docker hosts to communicate with each other. 
-> This is particularly useful in Docker Swarm or Kubernetes setups, where containers may run on different physical or virtual machines but still need to interact as though they are on the same network. 
-> This is typically achieved by encapsulating the original network packets in a new packet format and then transmitting them over the underlying network. 
-> Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other. 
-> You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. 
-> This strategy removes the need to do OS-level routing between these containers.

--> Key Features of Overlay Networks:
1. Cross-Host Communication: Overlay networks enable containers on different Docker hosts (servers) to communicate with each other over a secure network. 
-> This is a fundamental feature for multi-host networking, such as in a Docker Swarm or Kubernetes cluster.
2. Encapsulation and Encryption: Overlay networks encapsulate traffic between containers. 
-> This means that network traffic is encrypted and tunneled between Docker hosts using VXLAN (Virtual Extensible LAN) technology. This makes it secure and isolated.
3. Used with Docker Swarm: Overlay networks are commonly used in Docker Swarm to enable communication between services on different nodes in a Swarm cluster.
4. Works with Docker Compose: Overlay networks can also be used with Docker Compose in a multi-host configuration.

--> How Overlay Networks Work in Docker:
-> When using an overlay network, Docker creates a virtual network that spans multiple hosts. 
-> The containers on each host are connected to this virtual network, and Docker uses an underlying mechanism (such as VXLAN) to route traffic between them.

** Key Components:
1. Docker Swarm: When running in swarm mode, Docker automatically manages the creation and distribution of overlay networks.
2. VXLAN: This technology is used for creating a virtual tunnel that allows the encapsulation of network packets. 
-> It ensures that traffic between containers on different hosts is routed correctly.

Creating an Overlay Network in Docker:
1. Initialize Docker Swarm (if not already done):
docker swarm init

2. Create an Overlay Network: Once Swarm mode is initialized, you can create an overlay network using the following command:
docker network create --driver overlay my_overlay_network

3. Verify the Network: After creating the network, you can verify its creation with:
docker network ls

4. Now, you can run containers on this overlay network. However, containers must be running on different hosts (nodes in the Swarm cluster) to see the benefit of an overlay network. Here's how you would start containers using the network:
docker service create --name my_service --replicas 2 --network my_overlay_network nginx

------------------------------------------------------------------------------------------------------------------------------------------------------------
EXPOSE: exposing a port is done in dockerfile 
	$$ EXPOSE <port_number>
-> allows container to communicate with each other on the exposed port 
-> the service in the container is not accessible from the outside world but can be accessible inside the containers 
-> This is applicable for inter container communication 

PUBLISH: 
-> allow containers to talk to each other and  to the outside world 
	$$ PUBLISH = EXPOSE + PUBLISH 
-> the service inside the container is accessible from anywhere 

to publish a single port 
	$$ docker run -p <hostport>:<container_port> .....

to publish all the available exposed port of the container 
	$$ docker run -P ..............

Publshing range of ports 
many to many - if we want to publish range ports then the range must match the number of ports between the docker published ports and host ports 
	$$ docker run -p 8081-8085:9091-9095
one to many : 
we can specify range of host ports where docker automatically publshes the container port to the one of the available port on host with in the range of host port 
	$$ docker run -p 8081-8085:8080
------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker compose: V2.3.3
-> Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services.
-> Then, with a single command, you create and start all the services from your configuration.

to Install docker compose follow
 https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-22-04
 
and create a sample docker compose project.

version--->mandatory-version of docker compose mandatory
services--->mandatory-services you want to bring up
volumes--->optional
network--->optional
------------------------------------------------------------------------------------------------------------------------------------------------------------

docker compose up -d :to start docker compose file
docker compose logs-->to check logs generated by the compose
docker compose pause--->If you want to pause the environment execution without changing the current state of your containers
               unpause
docker compose stop--->The stop command will terminate the container execution, but it won’t destroy any data associated with your containers
docker compose down--->If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command
------------------------------------------------------------------------------------------------------------------------------------------------------------

docker-compose.yml  for bringing sonarqube with postgress db
-----------------------------------------------------------------------------------------------------------------------------------------------------------

version: "3"

services:
  sonarqube:
    image: sonarqube:lts
    container_name: sonarqube
    ports:
      - 9000:9000
    networks:
      - sonarnet
    environment:
      - SONARQUBE_JDBC_URL=jdbc:postgresql://db:5432/sonar
      - SONARQUBE_JDBC_USERNAME=sonar
      - SONARQUBE_JDBC_PASSWORD=sonar
    volumes:
      - sonarqube_conf:/opt/sonarqube/conf
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_extensions:/opt/sonarqube/extensions
      - sonarqube_bundled-plugins:/opt/sonarqube/lib/bundled-plugins

  db:
    image: postgres:11.5
    container_name: postgres
    ports:
      - 5432:5432
    networks:
      - sonarnet
    environment:
      - POSTGRES_USER=sonar
      - POSTGRES_PASSWORD=sonar
    volumes:
      - postgresql:/var/lib/postgresql
      - postgresql_data:/var/lib/postgresql/data

networks:
  sonarnet:

volumes:
  sonarqube_conf:
  sonarqube_data:
  sonarqube_extensions:
  sonarqube_bundled-plugins:
  postgresql:
  postgresql_data:
-----------------------------------------------------------------------------------------------------------------------------------------------------------

Q. what is the command syntax for retrieving container logs?
-> The docker logs command instructs Docker to fetch the logs for a running container at the time of execution. 
   It only works with containers utilizing the JSON-file or journald logging driver.
-> The command syntax for retrieving container logs is:
	$$ sudo docker container logs [option] container_id

Q. How can I see all logs of a docker container?
-> First of all, to list all running containers, use the docker ps command. Then, with the docker logs command you can list the logs for a particular container. Most of the time you'll end up tailing these logs in real time, or checking the last few logs lines.
   	$$ docker ps -q | xargs -L 1 docker logs

------------------------------------------------------------------------------------------------------------------------------------------------------------

** To copy a file from container to local machine.
sudo docker cp <Container ID>:<Path of file inside the container> <Path in the local machine>
docker cp 67d47d1ec280:/ram.txt ./

** To copy a file from local machine to container
sudo docker cp <Path in the local machine> <Container ID>:<Path of file inside the container>
docker cp ./ram.txt 67d47d1ec280:/etc
------------------------------------------------------------------------------------------------------------------------------------------------------------

Q. What is a .dockerignore file?
-> Similar to a .gitignore file, a .Dockerignore files allows you to mention a list of files and/or directories which you might want to ignore while building the image. 
-> This would definitely reduce the size of the image and also help to speed up the docker build process. 
------------------------------------------------------------------------------------------------------------------------------------------------------------

-> To limit the maximum amount of memory usage for a container, add the --memory/-m option to the docker run command. 
-> Within the command, specify how much memory you want to dedicate to that specific container. 
The command should follow the syntax: 
	$$ sudo docker run -it --memory="[memory_limit]" [docker_image]

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How will you monitor Docker in production?
-> Docker provides tools like "docker stats" and "docker events" to monitor Docker in production.
-> We can get reports on important statistics with these commands.

$$ Docker stats: When we call docker stats with a container id, we get the CPU, memory usage etc of a container. It is similar to top command in Linux.
$$ Docker events: Docker events are a command to see the stream of activities that are going on in Docker daemon.
-> Some of the common Docker events are: attach, commit, die, detach, rename, destroy etc.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How Multi-stage Build Works
-> With multi-stage builds, you use multiple FROM statements in your Dockerfile.
   Each FROM instruction can use a different base, and each of them begins a new stage of the build.
   We can selectively copy artifacts from one stage to another, leaving behind everything we don’t want in the final image.
-> In multi-stage build, we pick the tasks of building and running our applications into different stages. Here, we start with a large image that includes all of the necessary dependencies needed to compile the binary executable of our application. 
   This can be termed as the builder stage.
-> We then took a lightweight image for our run stage which includes only what is needed to run a binary executable. 
   i.e just having jre in our final stage is sufficient to run our application. This can be termed as the production stage.

-> We can use multiple FROM commands combined with AS commands in our Dockerfile where the last FROM command will actually build the image. 
   All the FROM commands before that, will lead to the creation of intermediate images which are cached regularly.
-> The AS command when used with the FROM command allows us to provide a virtual name for our intermediate images.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How container achieve isolation
-> Docker containers achieve isolation by leveraging (barrow, use something to maximum advantage) Linux features like "Control groups (commonly abbreviated as cgroups), secure computing mode (seccomp) filters, and kernel namespaces". 

Q. What are docker namespace and control groups?
-> When you start a container with docker run, behind the scenes Docker creates a set of namespaces and control groups for the container. 

1. Namespaces provide the first and most straightforward form of isolation: processes running within a container cannot see, and even less affect, processes running in another container, or in the host system.

2. Control group: A Control Group can be used to limit the number of resources that a particular process can use. 
-> A control group can be used to limit the amount of memory that a process can use the amount of CPU, the amount of hard drive input-output and the amount of network bandwidth as well.

------------------------------------------------------------------------------------------------------------------------------------------------------
My Docker Development Workflow: Code, Build, Push, Run

1. Write the code (and test it)
2. Build a container image
3. Push the image to the server
4. Restart the application, with the new image

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. What is monolithic appn and microservices and why are we moving from monolithic to microservices?
->  A monolithic application is built as a single unified unit while a microservices architecture is a collection of smaller, independently deployable services.
-> Because you can detect bugs or performance issues in a gradual fashion.

Q. How do I set an environment variable in Docker?
-> Use the --build-arg option
	$$ docker build --build-arg [arg-variable]=[value]
-> The output shows that Docker processed the ARG value and assigned it to ENV .

Q. How to take snapshot of container?
-> The command $$ docker commit takes a snapshot of your container. That snapshot is an image, which you can put on a (private) repository to be able to pull it on another host. 
-> An option that does not use an image (which you say you want to avoid) is indeed save and load.

Q. How many containers can be run per host?
-> Using this simple calculation, we can estimate that we can run about 1,000 containers on a single host with 10GB of available disk space.


======================================================================================
Q. what is out of memory error in docker?
-> If a container is using an unexpected amount of memory, it runs out of memory without affecting other containers or the host machine. 
-> Within this setting, if the kernel memory limit is lower than the user memory limit, running out of kernel memory causes the container to experience an OOM error.
-> However, it would be fine if the root user is used rather than the created user.
-> On Linux systems, if the kernel detects that there is not enough memory to perform some system task then it throws OOM or out of memory error and starts killing the processes to free up some memory. 
-> Any process can be killed by kernel. That means it can kill Docker and Docker containers as well. Which can be a high risk if your application is running on  production and suddenly due to out of memory error occurred , your container was killed and your production application faces some down time. So it is very important to understand and control this memory issue by docker.

Q. What we can do? 
1. We need to perform some analysis to understand the application's memory requirement before deploying it to production.
2. Make sure you are using a host which has sufficient resources to run your containers and application.
3. Control your container's behaviour and restrict it's too much memory consumption.
   $$ sudo docker run -it --memory="[memory_limit]" [docker_image]
   

=======================================================================================
Q. Is it possible to edit the base image?
-> It is possible to edit the base image of a container, but it is generally not recommended as it can lead to issues with compatibility and consistency.
-> A base image is the foundation of a container, it contains the operating system and any necessary libraries and dependencies. When you create a container, you can add additional files and configurations on top of the base image.
-> If you want to make changes to the base image, you can do so by creating a new image from the existing one using the "docker commit" command. However, this can lead to issues with compatibility and consistency.
-> A better approach is to use a versioned base image and update your container with the new version of the image. This way you can ensure that the changes made to the base image have been thoroughly tested and validated by the image maintainer.


================================================================================
Q. Deploying onto host. deploying onto container, which one u will prefer?
1. Deploying an application onto a host has the advantage of being simple and straightforward, as it does not require any additional infrastructure or configuration. 
-> It also allows for more flexibility in terms of resource allocation, as the application can use the resources of the host directly. 
-> However, this approach can lead to issues with compatibility and consistency, as different applications may have different dependencies and requirements.

2. Deploying an application onto a container, on the other hand, has the advantage of providing a consistent and isolated environment for the application to run in. 
-> This makes it easier to manage the application's dependencies and to ensure compatibility with other applications. 
-> Additionally, containerization can provide better scalability, portability, and security. 
-> However, this approach can be more complex and requires additional infrastructure, such as a container orchestration platform, in order to manage the containerized applications.
------------------------------------------------------------------------------------------------------------------------------------------------------------

Podman: An open-source tool that is similar to Docker in terms of functionality but does not require a daemon to run. 
        It is built to be more secure and is compatible with the Docker command-line interface.

CRI-O: A lightweight container runtime that is specifically designed to be used with Kubernetes. 
       It is built to integrate well with the Kubernetes API and is designed to be more secure and efficient than Docker.

containerd: An open-source container runtime that is designed to be lightweight and modular. 
            It is designed to be used as a component of other container orchestration platforms and 
            is used by projects such as Kubernetes and Docker itself.

rkt: A container runtime that is designed to be more secure and efficient than Docker. 
     It uses a different approach to container management and is designed to be more lightweight and less resource-intensive than Docker.

LXD: A container hypervisor that is similar to a traditional virtual machine hypervisor. 
     It uses Linux Containers (LXC) to provide isolated, lightweight environments that are similar to virtual machines.

Firecracker: A microVM runtime that is designed to be lightweight and secure. 
             It is built for running serverless workloads and is used by AWS for its Lambda and Fargate services.


------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. What is depends_on in docker
-> depends_on is a Docker Compose keyword to set the order in which services must start and stop.

Q. How to Import docker container in Kubernetes?
-> you can use the kubectl command-line tool to create a new Kubernetes pod and run the Docker container inside it.
1. Pull the Docker container image: first need to download the Docker container image from a registry. You can do this using the docker pull command.
2. Create a Kubernetes pod: Once you have the Docker container image, you can use the kubectl run command to create a new Kubernetes pod 
   that will run the Docker container. The kubectl run command creates a deployment and a pod, so you don't need to create a deployment and a pod separately.
3. Specify the Docker container image: When you create the Kubernetes pod, you need to specify the Docker container image that you 
   want to run. You can do this by using the --image option with the kubectl run command.
4. Expose the pod: By default, pods in Kubernetes are only accessible from within the cluster. To make the pod accessible from outside the cluster, 
   you need to expose it using a Kubernetes service. You can use the kubectl expose command to expose the pod as a service.
5. Verify the pod is running: You can use the kubectl get pods command to verify that the pod is running. You should see the pod in the list of running pods.

Q. What are the containers? which are running?
-> Containers are a form of lightweight, stand-alone, and executable software packages that include everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and config files.
   $$ docker ps : list the running containers
------------------------------------------------------------------------------------------------------------------------------------------------------------

Write a Dockerfile in which you have to pull the image from the registry and set hardcoded environment variable and then
copy some dependices file to filesystem of container and then create a directory to somepath
and copy projectfile to newly created directory and give read,write and execute permission to newly directory
and copy another one docker script and make that script executable.
Dockerfile:
==========
# Pull the base image from the registry
FROM registry-image:latest

# Set an environment variable
ENV MY_VAR=my_value

# Copy dependencies file to the container filesystem
COPY dependencies.txt /app/dependencies.txt

# Create a directory and copy project files to it
RUN mkdir -p /app/project
COPY projectfile /app/project/projectfile

# Give read, write, and execute permissions to the project directory
RUN chmod 777 /app/project

# Copy the Docker script to the container filesystem and make it executable
COPY my_script.sh /app/my_script.sh
RUN chmod +x /app/my_script.sh

# Set the working directory to the project directory
WORKDIR /app/project

# Start the container by running the script
CMD ["/app/my_script.sh"]
