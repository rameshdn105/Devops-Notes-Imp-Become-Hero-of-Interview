Q. What is monolithic appn and microservices and why are we moving from monolithic to microservices?
-> A monolithic application is built as a single unified unit while a microservices architecture is a collection of smaller, 
   independently deployable services.
-> Because you can detect bugs or performance issues in a gradual fashion.
-> In microservices small services can be written in different languages(Python, Node.js, Java) but where as in monolithic its more of a single language.
-> Microservice would be cost efective as it can manipulate servers depending on the traffic, but in monolithic it has to create a lot number 
   of services as per traffic.
-> Ex for microservice: Kubernetes.

monolithic: modules, together deployed with tight coupling
through ansible, automation jenkins
downtime high

microservices: deploy k8s, modules will be individually
no downtime 
i.e, downtime other application is not possible
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Definition of Docker Swarm:
-> The Docker Swarm is essentially a kind of tool which allows us to create and schedule the multiple docker nodes easily. 
-> The docker swarm can also be used for a vast number of docker nodes. Each Node in the docker swarm is itself actually a docker daemon, and that demon is 
   able to interact with the Docker API and has the benefits of being a full docker environment.
-> Docker Swarm can reschedule containers on node failures. 
-> Swarm node has a backup folder which we can use to restore the data onto a new Swarm.

# Features of Docker Swarm
1. Decentralized access: Swarm makes it very easy for teams to access and manage the environment 
2. High security: Any communication between the manager and client nodes within the Swarm is highly secure 
3. Autoload balancing: There is autoload balancing within your environment, and you can script that into how you write out and structure the Swarm environment allowing you to increase or decrease the number of replicas of a service as desired. 
4. High scalability: Load balancing converts the Swarm environment into a highly scalable infrastructure
5. Rolling updates: Docker Swarm enables easy rolling updates.

#  What are Docker Swarm Nodes? 
-> A docker swarm can recognize three different types of nodes, and each type of Node plays a different role within the ecosystem of the docker swarm.
#  Types of Nodes: Leader Node, Manager and Worker node
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes vs. Docker Swarm
The table below illustrates the differences between Kubernetes vs. Docker Swarm:

   Features				Kubernetes							Docker Swarm
-Installation&clusture		Complex, clustre is strong					Simple, clusture is not strong
-GUI				GUI is K8s dasboard						There is not GUI
-Autoscaling			It can do auto scaling						Can do autoscaling, but slower
-Scalability			Scaling and deployment are comparatively slower 		Containers are deployed much faster
-Logging & monitoring		In-built tool loging & montoring				3rd part tools like ELK (Elasticsearch, Logstash, Kibana) 															should be used
-Rolling updates & Rollback	both it can do							Can only do Rolling updates
-Load Balancing			Manual intervention is required					Automated load balancing
					for load balancing 
-Cluster			Difficult to set-up						Easy to set-up
-Container Setup		Commands like YAML should be rewritten 				A container can be easily deployed to different platforms
					while switching platforms
-Availability			High availability when pods are distributed 			Increases availability of applications through redundancy
					among the nodes
-Data volumes			Shared with containers from the same pod			Can be shared with any container

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes-is known as K8S-Container orchestration tool
Kubernetes, is an open-source system for automating deployment, scaling, and management of containerized applications.  
            It groups containers that make up an application into logical units for easy management and discovery.
-> daemon service: 100% of service is available.
-> -> Monitor K8S: -> Prometheus, Grafana, Fludentd, Elasticsearch, Kiabana, Stackdriver, New Relic, Datadog.

Q. Which laguage k8s is written?
-> K8s, is written primarily in the "Go" open source programming language or "Golang". It is known for its simplicity, speed, and efficient memory management.

Kubernetes Features-
1)Self-healing--> Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don't 
       respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve. (Probe)
2)Automated rollouts and rollbacks--->Kubernetes progressively rolls out changes to your application or its configuration, 
       while monitoring application health to ensure it doesn't kill all your instances at the same time. 
      -> If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.
3)Horizontal scaling--->Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.
      Horizontal scaling means that the response to increased load is to deploy more Pods.  *vertical scaling : increasing CPU usage/Hardware capacities.
4)Service discovery and load balancing--->No need to modify your application to use an unfamiliar service discovery mechanism. 
       Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

-> Alternatives: Amazon ECS(Elastic container service), Docker enterprise, Azure Kuberntes service, Google Kubernetes Engine (GKE), Redhat, Portainer, Saltstack, Rancher
-> problem statement- running multiple containers, scaling, managing many containers, when they fail to run,support and communicate
* Available tools
1. docker swarm
2. Apache mesos
3. Kubernetes

Q. How to monitor k8s clusture?
-> Using built-in tools: kubectl, kubectl top, and kubectl logs (provide information about resource usage, pod status, and logs)
-> Using Prometheus and Grafana: Prometheus is an open-source monitoring and alerting system, while Grafana is an open-source visualization tool.
-> Using commercial monitoring tools: Datadog, New Relic, and AppDynamics, provide advanced monitoring and visualization capabilities for Kubernetes clusters.
-> Using log aggregation and analysis tools: Log aggregation and analysis tools, such as Elasticsearch, Logstash, and Kibana (ELK), 
   can be used to collect and analyze logs from the cluster, providing a centralized view of log data.
kubectl pod: list running nodes and pods along with resource utilization

Methods of installing Kuburnetes: tyes of clusture available in market, types of k8s clusture
1.HArd Way
2.Kubedin, (both for experimental production grade cluster)can also be used for production
3.MiniKube: easy to run a single-node Kubernetes cluster locally on your machine.
4.Managed K8s (Amazon Elastic Kubernetes Service, Azure Kubernetes Service, Oracle Kubernetes Service, Google Kubernetes Engine)

Q. how to setup kubernetes locally?
1. Minikube: easy to run a single-node Kubernetes cluster locally on your machine. ($$ minikube start)(Windows, Linux, and macOS)
2. Docker for Desktop: Docker installed on your machine, you can use Docker for Desktop to set up a local Kubernetes cluster. (kubectl commands to interact with the cluster)
3. Microk8s: is a lightweight, fast, and simple Kubernetes distribution that runs natively on Linux. Run a local cluster on your machine with the command microk8s start.   
4. k3s: k3s is a lightweight Kubernetes distribution that is designed for resource-constrained environments.
5. kind (Kubernetes in Docker): It is a tool for running local Kubernetes clusters using Docker container "nodes". (create a cluster with a single command kind create cluster)

-------------------------------------------------------------------------------------------------------------------------------------------------------
-> Container orchestration is the automation of much of the operational effort required to run containerized workloads and services.
   This includes a wide range of things software teams need to manage a container's lifecycle, including provisioning, deployment, scaling (up and down),
   networking, load balancing and more

-------------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes Architecture and Components:

Kubernetes Master components:
DEVOLOPERS===>API Server--->(Key value store-etcd :: controllers :: Scheduler)====> Master node
											|
	     Kubelet---->Container runtime----> Network proxy---->Pod1, Pod2...====> Worker node
											|
										       USERS

Kubernetes Master Node: 
CLI/UI-->API-->(API server::SCHEDULER::CONTROLLERS::KEY VALUE STORE-etcd)

Q. What is Master Node in Kubernetes Architecture?
-> The Kubernetes Master (Master Node) receives input from a CLI (Command-Line Interface) or UI (User Interface) via an API. 
   These are the commands you provide to Kubernetes:
-> You define pods, replica sets, and services that you want Kubernetes to maintain. For example, which container image to use, 
   which ports to expose, and how many pod replicas to run.
-> You also provide the parameters of the desired state for the application(s) running in that cluster.

*kubectl: Command line interface, it is tool to connect between api server and user.
*Kubeadm is a tool used to build Kubernetes (K8s) clusters. Kubeadm performs the actions necessary to get a minimum viable cluster up and running quickly.
	$$ kubeadm validate clusture 
*kubeconfig is a configuration file used by kubectl, to connect to a Kubernetes cluster. (easily switch between different clusters and users in the same configuration file)
-The kubeconfig file contains information such as the location of the API server, the credentials used  to authenticate with the API server,
 and the current context (the cluster, user, and namespace that kubectl is currently interacting with).
-usually located in the user's home directory under ~/.kube/config but specified with KUBECONFIG environment variable or the --kubeconfig flag.
 

API server: (enables the communication)
		- API server is the main management point of the entire cluster.
		- The API Server is the front-end of the control plane and the only component in the control plane that we interact with directly.
		* When ever user interact with the kubernetes cluster the kubectl command actually communicates with the master API server.
		* All the components in the k8s cluster communicates through API server, no other component can communicate with each other directly.
		* API server is also responsible for authentication and authorization.	
		* API server has got the watch mechanism to watch the changes in the worker nodes.
		* API server is the heart of the k8s cluster most of the decision are done by API server
*what process used to run k8s master node?   ans: API server.
		   	
Key-Value Store (etcd):  (It stores the current information)
		*(By default, etcd data is stored in folder /var/lib/etcd , which is most likely stored on the root file system)
		* etcd is a distributed, consistent key-value store which stores the configuration data of the complete Kubernetes cluster.
		- configuration data represent the desired state of the cluster Ex:-
			- which nodes exist in the cluster
			- what are the pods running, and on which node they are ruuning on 
			- whole lot information on the cluster.
			- To backup the cluster we need to backup the etcd
-> Secure: (Authentication, Encryption, Authorization, Network segmentation, Backup and restore, Monitor and Audit, Update and patch)

Scheduler: (decides where our container will be running on which node)
		- The scheduler is the one which decides which pod should be created on which worker node.
		- Always watches the unscheduled pods/ new pods and binds them to a worker node
		  based on the availability of the requested resources, service requirements, affinity 
		  and anti-affinity specifications and other constraints.
				  
Controller manager: (responsible for monitoring worker node, your containers and authentication autorization.)
		- Controller manager is a daemon which always runs the core control loops known as controllers. 
		- Controller watches the state of the cluster through the API server watch feature.
		- When it get notified, it makes to move current state towards the desired state.
		
Kubernetes worker node components:
kubelet: (it does heavy lifting on container, fetch image, map volumes, run containers)
		- The kubelet is the most important component of k8s in worker node.
		* It is the main agent that run on each worker node and communicates with API server to apply the desired state to the node.
        * It sends all the metrics of the worker node to API server using a tool called cAdvisor.
		* It communicates with the docker daemon using docker socket api to create pods and to change the configurations of pods.

		The main responsibilities of kubelet are:
			- Run/create the pod with container runtime.
			- Always reports the status of pods to API server.
			- Reports the current status of worker node and pods to API server
			
kube-proxy: (expose pod to external world we can use kube-proxy, or we can set network rules)
		* Also known as service proxy.
		* kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.
		* Is responsible for watching the Kubernetes API server on master for changes on service and pod network configuration.
		* These rules allow the pods, nodes and pods in different nodes of cluster to communicate with each other.  
		* This monitors the assignment of IP address to pods.
		* Entire network configuration is maintained by service proxy.	
			
container runtime: 
		* software that is responsible for running containers.
		* Docker: default runtime when no other runtime is specified. (provides a robust and well-documented container runtime)
		* Kubernetes support several conatainer runtime: Docker, conateinerd, CRI-O (Container Runtime Interface for OpenShift)
		- In our company we are using Docker as containerization technology.
		
1. containerd is an industry-standard container runtime that is designed to be lightweight and modular, 
   it is used as the default runtime in some Kubernetes distributions like Rancher and OpenShift.
2. CRI-O is a Kubernetes-specific container runtime that is optimized for Kubernetes, it can work with both containerd and runc.
3. rkt is an alternative container runtime that is designed to be simple, secure and composable.

-> cAdvisor is an open-source agent integrated into the kubelet binary that monitors resource usage and analyzes the performance of containers. 
   It collects statistics about the CPU, memory, file, and network usage for all containers running on a given node. 
   (it does not operate at the pod level).

*docker engine: where containers are running.

----------------------------------------------------------------------------------------------------------------------------------
kubeadm join k8smaster.example.net:6443 --token gyqs21.gdqeh7suxy5o5o8p \
        --discovery-token-ca-cert-hash sha256:1591ce53963b60ca9fdd42bdf27975db7ca10d660fac14c13bb1fc287614a0b0
----------------------------------------------------------------------------------------------------------------------------------------
List deployments 
$ sudo kubectl get deployment
				
List pods 
$ sudo kubectl get pods
				
Try deleting a pod 
$ sudo kubectl delete pod <pod-name>
k8s should automatically create pods to maintain the replicas

Run the below command to create  containers from yaml	
$ sudo kubectl apply -f example.yaml

oneliner to create deployment with 2 replicas
$ kubectl create deployment nginx-app --image=nginx --replicas=2

to expose port 80
kubectl expose deployment nginx-app --type=NodePort --port=80

Q. What happens when I create a pod/ Workflow of Kubuernates ?
1. kubectl writes to the API Server.
2. API Server validates the request and persists it to etcd.
3. etcd notifies back the API Server.
4. API Server invokes the Scheduler.
5. Scheduler decides where to run the pod on and return that to the API Server.
6. API Server persists it to etcd.
7. etcd notifies back the API Server.
8. API Server invokes the Kubelet in the corresponding node.
9. Kubelet talks to the Docker daemon using the API over the Docker socket to create container.
10. Kubelet updates the pod status to the API Server.
11. API Server persists the new state in etcd.

<-->    API server--etcd
kubectl--API server<-->etcd--Scheduer<-->Kubelet--DockerDaemon (Create container)<-->

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
pod 
	- pods are the smallest deployable objects in kubernets.
    - pods should contain atleast one container and may contain many containers.
	- pods are ephemeral(if pod fails to work Kubernetes will produce a replica of that pode: Self helaling nature) by nature. 
    - If a pod or a node in which it executes if it fails then kubernetes can automatically create a new pod replica to continue the operations. 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------	
Kubernetes configuration / desired state 
apiVersion: v1
kind: Deployment 
metadata:
  name: static-web
  labels:
	role: myrole
spec:
  containers:
	- name: web
	  image: nginx
	  ports:
		- name: web
		  containerPort: 80
		  protocol: TCP
		
	
	 apiVersion
		- we need to specify which version of Kubernetes API to be used 
		  to create a object.
		- Kubernetes provides stable, alpha and beta versions of api.
			
	 kind
		- Specify the kind of kubernetes object that need to be created.
		- Object name first letter should be in uppercase.
		
	 metadata
		- Specify the information about the object so that it can be 
		  uniquely identified by other objects.
	 
        spec
		Specify the configuration to define the desired state of the object.

To create an object from a kubernetes spec file 
	sudo kubectl create -f <spec_yaml_path>

To apply the changes of spec file to the object 
	sudo kubectl apply -f <spec_yaml_path>
	
To list the objects in kubernetes 
	sudo kubectl get <object_type>

To delete an object 
    sudo kubectl delete <object_type> <object_name>

To check the details of object 
	sudo kubectl describe <object_type> <object_name>	
	
--------------------------------------------------------------------------------------------------------------------------------------------
Labels 

	- k8s labels are applied to objects which allow to identify, select and operate on objects with label applied.
	- labels are key value pairs that can be applied/attached to pods, services, deployment, DaemonSet, nodes etc.
	- keys are defined by kubernetes and it is not user-defined.
	- Label key should not conatin special character.
	- Values can include dot, but start and ending can only be alpha numeric.
	
	To list the labels of a pod 
		sudo kubectl get pod <pod_name> --show-labels 

	To list the labels of a object 
		sudo kubectl get object <object_name> --show-labels
		
	Add a label to a pod 
		sudo kubectl label pod <pod_name> <label_key> <label_value>	(Using YML also we can label)
		
	Add a label to a node
		sudo kubectl label node <node_name> <label_key> <label_value>
		
	To give multiple labels
			- app: nginx-deployment
			- tier: frontend
				
	Diff between labels and metadata
		labels are user-defined and metadata are pre-defined
		
=======================================================================================================================================
Selectors:
	- selectors help us to identify/filter out the objects using matching labels of the objects.
	
	*Equality Based selector 
	   	- comparision is based on only equality or inequality.
		- Three kinds of operators that I can use is = or ==, != 
		- Can only check single set of values.
			ex: app = front-end or app == front-end 
				environment != prod
        kubectl get pods -l environment=production,tier=frontend
				
	*Set-Based selector 
		- It allow us to filter resources according to set of values.
		- Three kind of operators in, notin and exists.
				ex: app in (front-end, back-end)
       kubectl get pods -l 'environment in (production),tier in (frontend)'	(-l:labels)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
Replica Set vs Replication Controller: 	
  
	- Both are used to create defined replicas of pod at a given point of time.
	- These objects can be used idividually.
	- Deployments always uses Replica Set not Replication Controller.
	- Replica Set selects resources with set-based selectors.
	- Replication Controller selects resources with equality based selectors
	- To check Replication Controller used 
			kubectl get rc
	- To check Replica Set used 
			kubectl get rs

The major difference between a replication controller and replica set is that the rolling-update command works with Replication Controllers, 
but won't work with a Replica Set.  This is because Replica Sets are meant to be used as the backend for Deployments.

---------------------------------------------------------------------------------------------------------------------------------------------------------
Default Controllers of kubernetes:
-> When we install kubernetes we will get some controllers by default and 
-> It is used by kubernetes only.
		
Node controller:
	- Looks for node status and responds to API server when a node is down
          EndPoint Controller: 
	- Populates the information of endpoint objects (pods, services, jobs, deployment,replicas ...)
    	  
Service Discovery: 
	There are 2 ways to discover a service 
	DNS (This is recommended method,   DNS:Domain name server)
		 - The DNS server is added to the cluster in order to watch the kubernetes
		   API create DNS record sets for each new service.
		 
    ENV var 
		- pod runs on a node, so that the kubernetes adds environment variables 
		   for each active service (each pod).

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
Differnt type of Controllers:
Deployment:
    * To create or modify instances of the pods that hold a containerized application.
	* Deployment can maintain multiple set of pods at a given point of time using ReplicaSet.
	* Deployment watches whether all the instances of pod is running or not, if not running deployment will create a new pod instance to
	  maintain the number of replica using ReplicaSet.
	* Deployment makes Scaling of pods easy, by changing the number of pods we need at a given point of time.
	* We can easily expose a pod to the outside world means outside the cluster.
	* We can rollback to an earlier deployment version.
	* We can also manage the states of the pod paused, edited and roolbacked.
	* Rolling and rollback of updates to all pod instances using deployment is easy.
	
	scaling: 
		- we can change the value for number of replicas in spec file.
		- we can also use th below command 
			sudo kubectl scale deployment.v1.apps/nginx-deployment --replicas=3
	
	autoscalling (deployment internal autoscaller)
		- kubernetes can scale deployment automatically based on resource usage.
			sudo kubectl autoscale deployment.v1.apps/nginx-deployment --min=4 --max=20 --cpu-percent=80	
	
       *vertical scaling : increasing CPU usage/Hardware capacities.
	
	rollback the deployment:
	    - Sometimes, you may want to rollback a Deployment; For example, when the Deployment is not stable, such as crash looping.
	      By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want.
	      (you can change that by modifying revision history limit).
		  
		- To check rollout status
			kubectl rollout status deployment/nginx-deployment
			
		- Check the old replicas
			kubectl get rs
			
		- kubectl descibe deployment (Command)
		
		# To rollout:
			- First, check the revisions of this Deployment:
					nginx-deployment
				
			- To see the details of each revision, run:
				kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
				
			- Now you've decided to undo the current rollout and rollback to the previous revision:
				kubectl rollout undo deployment.v1.apps/nginx-deployment
				
			- Alternatively, you can rollback to a specific revision by specifying it with --to-revision:
				kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2

Q. how do you delete deployment?      
-> $$ kubectl delete deployment <deployment name>
   $$ kubectl delete object <object name>

DaemonSet:			
	- A DaemonSet ensures that a copy of a pod is always running on all the nodes in the cluster.
	- If a new node is added/removed from cluster then DaemonSet will automatically adds/deletes the pods from that node.	  
Usage: 
	1) Monitoring agents: we need a agent called node exporter to be running in every worker node to monitor all the nodes in cluster.
    2) Logs collection Daemon: If we want to export logs of node and pods running in it we can create that log exporter using DaemonSet.				
Limitations:
		1) it does not automatically run on any node which has taint.
		Ex: Master, we need to specify the tolerations for it on the pod.
Monitoring tools: Prometheus, Grafana

StatefulSet:
	- StatefullSets, just like deployment controller, creates the pod according to that manifest and replicas mentioned it in. 
	- StatefulSet gives the pods with a unique identity and well defined name for the pods to address each other.
	- StatefulSets maintains the sticky identity for each pod which will remain even if the pod gets killed and new pod replaces it. 
	  
    - It creates the Stateful application:
    - For the StatefulSets to work, a headless service should be created. 
    - Also StatefulSets need persistent data storage, so that the application saves the data and states across the restarts. 	 
    - StatefulSets are recommended when running Cassandra, MongoDB, MySQL, PostgreSQL or any other workload utilizing persistent storage. 
    - They can help maintain state during scaling and update events, and are particularly useful for maintaining high availability.	  
	  
ReplicaSet 
	- A ReplicaSet is a set of multiple, identical pods with no unique identities. 
        - ReplicaSets were designed to address two requirements:	  
		•Containers are ephemeral. When they fail, we need their Pod to restart.
		•We need to run a defined number of Pods. If one is terminated or fails, we need new Pods to be activated.						
	- A ReplicaSet ensures that a specified number of Pod replicas are running at any given time.	  

Jobs	
	- Jobs creates the pods to carry out short lived workloads which might be performing a single task. 
	- Once the task assigned to the pod completes it will shut down by itself.
	  
	- Few tasks these Job Pods do are:
		a.Running a migration activity
		b.Rotating logs
		c.One time initialization of resources.
			-In case any pod or node fails in between the task the job controller ensures new pod/node is  
			 created as replacement and resumes the activity from there.		
			-Jobs can also run multiple Pods in parallel, giving you some extra throughput.
			
CronJobs
	- CronJobs create the pods to run the jobs in user defined schedule.
	- The schedule can set using the Cron syntax and every time the requires the job to run a pod wil be created and
	  after the job succeed the pod goes to shutdown state. 
	- Eg 
		- Taking back up of any application at a scheduled time everyday.
		- The CronJob controller also allows you to specify how many Jobs can run concurrently,
	          and how many succeeded or failed Jobs to keep around for logging and debugging
	
		
----------------------------------------------------------------------------------------------------------------------------------
Stateful Applications 
	- stateful applicaiton saves the user session data at the server side.
	- if server goes down it is difficult to transfer the session data to other server.
	- This type of applicaiton will not work, if we want to implement 
	  autoscalling for our applicaiton.

Stateless Applications
	- user session data is never save at the server side.
	- using a single authentication gateway or client token method to 
	  validate the users once for multiple microservices.

----------------------------------------------------------------------------------------------------------------------------------
	  
# Connect to Container in a POD
kubectl exec -it <pod-name> -- /bin/bash
kubectl exec -it my-first-pod -- /bin/bash

#To Get complete pod definition YAML output
kubectl get pod <podname> -o yaml 

#to get IP of all Pods
kubectl get pods -o wide

----------------------------------------------------------------------------------------------------------------------------------
Pods
	- Pod is a basic building block of Kubernetes.
	- That is the least category object. 
	- It’s a wrapper created by Kubernetes to work with container. 
	- Pod will have 1- n number of containers.

Lifecycle of pod phases / status
	1. pending 
		- It will wait for kubernetes cluster to accept.
		- It will wait till all the containers in pod to be running.
		- pod will spend some time waiting to be scheduled to a node and 
		  in downloading container images over the network.
	
	2. Running 
		- The pod has been bound to a node by scheduler.
		- All of the containers have been created.
		- At least one container is running or it may be in starting or restarting.
	
	3. Succeeded
		- All the containers in the pod have been terminated successfully.
		- No container will be restarted.
	
	4. Failed 
		- All the containers in the pod are not be running and any one 
		  container have been terminated in failure.
		
	5. Unknown
		- For some reason the state of the pod could not be identified or obtained.
		- This status may also occur if kubernetes cannot communincate with pod or node.
	
	Terminating 
		- when a pod is being deleted.
		- This status is not one of the pod phases.

===========================================================================================================================================================
Pod patterns / Container types 	
1. Init containers: 
	- Init containers are the containers that should run and complete
	  before the startup of the main container. (applicaiton container)
	- It provides a sperate lifecycle at initialization point.
	- can have multiple init contianers. init containers always run to completion. 
          each init container must complete succesfuly before the next one starts.
When to use this pattern ?
		- when our main container needs some prerequisites such as 
		  installing some supportive softwares, database setup, seeding the DB,
                  permissions on the file system before starting.
		- we can use this pattern to delay the start of the main conateiner.
		
	spec:
	  initContainers:
		- name: fetch
		  image: mwendler/wget
		  command: ["wget","--no-check-certificate","https://sample-videos.com/sql/Sample-SQL-File-1000rows.sql","-O","/docker-entrypoint-initdb.d/dump.sql"]
		  volumeMounts:
			- mountPath: /docker-entrypoint-initdb.d
			  name: dump

2. Sidecar containers: 
	- These are the containers that will run along with the main conateiner.
	- We have a pod with a main container which is working very well but 
	  If we want to extend the functionality without changing the existing 
	  main container then better option is to use sidecar conateiner.
	- To take the copy of file or data from main container we can use 
	  sidecar conateiner.

3. Single Container per Pod
4. Ambassador pattern
5. Adapter pattern
6. Batch Job pattern
7. DaemonSet pattern

-----------------------------------------------------------------------------------------------------------------------------------------------
Pod to pod communication 
    - By default all pods running in a node within a same namespace can communicate with each other without any configuration.
    - A pod in one worker node can access all the pods in the cluster which are in same namespace.	  
-> containers within same pod can comminicate each other .can accessed using localhost:port
-> containers in diffrent pod and node can be accessed using POD IP (Internet Protocol)

How to access containers from outside cluster???????
Kubernetes services:
	- service is an REST API objects with set of rules/policies for accessing set of pods.
	- services are always created and works at cluster level, not at node level.
	- services always points to pods directly using labels.
		
	- To list services 
		sudo kubectl get svc 
		
	- Why do we need service?
		- Kubernetes pods are ephemeral in nature. 
		- For eg. 
		     - The deployment object can create and destroy pods dynamically. 
	             - The set of pods running changes all the time, so even the IP address also. 
		     - The service provides static IP address through which the dependent pods/external requests can read the pods.

-----------------------------------------------------------------------------------------------------------------------------------------------
Q. What are type of loadblancing?
-> In Kubernetes, there are two types of load balancing: internal load balancing and external load balancing.
1. Internal load balancing: This is used to distribute traffic within the cluster. It allows pods within the cluster to access 
   services by their IP addresses, but it does not expose the service to external traffic. 
   The ClusterIP service type is an example of internal load balancing.
2. External load balancing: This is used to distribute traffic from outside the cluster to the pods within the cluster. 
   It exposes a service to external traffic by mapping it to a load balancer that is provisioned in the underlying infrastructure 
   such as cloud provider's load balancer. The LoadBalancer service type is an example of external load balancing.

Sevices brief:
Cluster ip: Cluster ip is to access internally within cluster, Pod to pod communication.
Node port: to allow my service/application present in clusture to be accessed by outside world. (30000-32767)
Headles service: Here we mention node port as none, so all ips of pods will be listed we can select and run the application, used with StatefulSets
loadbalancer: Here user traffic will be taken by loadbalancer and it will equally distribute it on target nodes.
Ingress service
ExternalName: DNS  (Domain name server)

--------------------------------------------------------------------------------------------------------------------------------------------------			
ClusterIP:	(Pod to pod communication)
-> It is the default type of Kubernetes service which exposes the pod IP to the other pods within the cluster.
-> This service is accessed using Kubernetes proxy.
-> Used to solve ephemeral nature of pod to avoid tracking of pod ip we create clusture ip to access internally within clusture.
	  
-> best option include service debugging during development and testing, internal traffic, and dashboards.
		  
	apiVersion: v1 
	kind: Service 
	metadata:
		name: my-ci
	spec:
	    selector: 
	     app: nginx
		type: ClusterIP
		ports:
			- name: http 
			  port: 30081
			  targetPort: 80
		      protocol: TCP	

NodePort:
	- A NodePort service is the most primitive way to get the external traffic directed to our 
	  service or application running inside our cluster.	
	- The default LoadBalancer of kubernetes is NodePort.
	- Applications running inside the pod will be exposed to the outside world with the 
	  use of NodePort and NodeIP, which expose the port on every node.
	- If we wont specify any port while defining NodePort, kubernetes
          will automatically assigns ports between the range 30000 - 32767
	- Automatically ClusterIP will be created internally.
	     clusterIP + a port mapping to the host port = NodePort
		 
	- NodePort by default opens the specified port in all the worker nodes in the cluster.  
	  
	- Types of ports involved are 
		- targetPort – port on the pod (service is forwarded to here)
		- port – port on the service itself
		- nodePort – port on the node (valid range for node port 30000 – 32767)

	apiVersion: v1 
	kind: Service 
	metadata:
		name: my-ci
	spec:
		selector: 
			app: nginx
		type: NodePort
		ports:
			- name: http 
			  nodePort: 30081	
			  port: 8080
			  targetPort: 80
		      protocol: TCP
			  
		kubectl describe svc <servicename>
	  
Headless services:
	- when we neither want load-balancing and a single service IP, use headless service.
	- Headless service lists all the ip's of the pods it is pointing, when a DNS query for headless service is run.	   
	- We can create a headless service by specifying none for the clusterIP.
	- Headless service is used with StatefulSets where name of the pods are fixed.
	
		apiVersion: v1 
		kind: Service 
		metadata:
			name: headless
		spec:
			selector: 
				app: nginx
			clusterIP: None
			ports:
				- name: http 
				  port: 30081
				  targetPort: 80
				  protocol: TCP
			
	 Note: To check the internal working 
			- Login to one of the pod in the group
			- Do nslookup on the services (clusterIP, NodePort and Headless)
			  			  
LoadBalancer:
	- Used to link the external load balancer functionality to the cluster.
	- Typically implemented by a cloud provider and mainly depends on the cloud provider.
        - A network load balancer with an IP address can be used to access the service. 
	- This is not a cost effective way of redirecting the traffic to the cluster.
	- Kubernetes provides a better alternative to this service which is called Ingress Service.  

Ingress service:
-> Ingress is an API object that provides routing rules to manage external users' access to the services in a Kubernetes cluster, typically via HTTPS/HTTP. 
1. easily set up rules for routing traffic without creating a bunch of Load Balancers or exposing each service on the node. 
2.  best option to use in production environments.
 
-> In production environments, configure and manage content-based routing, support for multiple protocols, and authentication inside the cluster. 
-> An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, 
   terminate SSL / TLS, and offer name-based virtual hosting 
   
-> In Kubernetes, an Ingress backend is a service that the Ingress controller routes traffic to based on the rules defined in an Ingress resource.
-> The backend service is defined as an object in Kubernetes, such as a Service, and is referenced in the Ingress resource by its name. 
   The backend service can be a deployment, a replica set, or any other resource that exposes a network endpoint.
								   Pod
								  /
clinet---Ingress-managed---->*Ingress*----routing rule--->service
          load balancer			  \
								   Pod
-> Ingress controllers are typically implemented as a reverse proxy, such as Nginx, Traefik, HAProxy, and Istio.

-> An Ingress provides the following:
1. Externally reachable URLs for applications deployed in Kubernetes clusters
2. Name-based virtual host and URI-based routing support
3. Load balancing rules and traffic, as well as SSL termination
-> Here is a simple example where an Ingress sends all its traffic to one Service:	  

## The Ingress resource ##
-> A minimal Ingress resource example:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: vv                                                                                                                                                                                                                                                                                                                                                                                                                 
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80

ExternalName:
	– Used to define the external DNS (Domain name server) name like cname record or IP address.
	- Maps the Service to the contents of the external Name field (e.g. foo.bar.example.com), 
	  by returning a CNAME record with its value. No proxy of any kind is set up.
	- Eg 
		– if the frontend application is hosted within the cluster 
		and the Database service is hosted externally and in the application
		if we can define the code to access the application like mangoDB 
		and still if want to access it internally then we can define it in External name. 
		
		apiVersion: v1
		kind: Service
		metadata:
			name: MangoDB
		spec:
			type: ExternalName
			externalName: mongoserverDNS name

Istio mesh: (managing microservices)
-> It provides a set of features for managing and securing microservices applications. 
-> It is built on top of the Kubernetes platform and is designed to work seamlessly with it.
-> An Istio service mesh is a configurable infrastructure layer for microservices application that makes communication flexible, reliable, and fast.
## It provides a number of key features, including:
1. Traffic management: controls flow of traffic between microservices, including features such as load balancing, traffic shaping, and fault injection.
2. Service discovery: provides a service discovery mechanism to easily discover and communicate with each other.
3. Security: provides built-in security features, such as mutual Transport Layer Security (TLS) and access control, to help secure communication between microservices.
4. Observability: provides a set of tools for monitoring and troubleshooting microservices, including metrics, logging, and tracing.
5. Configurable: control plane allows to configure and manage the service mesh, and to automate many tasks such as traffic management, security, and observability.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Pod Probes: 
	- Probes are used to obtain the health of an applicaiton running inside a pod's container.
	- Probes can perform periodic call to some applicaiton endpoint within a container which can track the
	  success or failure of application periodically.
	- When subsequent fails occur some user defind triggers can be done with probes.
	
	Advantages of probes 
		- Enable zero downtime deployments.
		- Prevent deployment of broken images.
		- Ensure that failed container are automatically restarted.
		- Can add a delay in starting the application.

Types of probes:
Startup Probe: (initial start of container, gives min startuptime, adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet)
	- This probe will run at the initial start of the container and gives a minimum startup 
	  time before running the another probes (liveness, rediness).	
	- Startup probe is the first probe which will be executed among other probes. 
	- This can be used to adopt liveness checks on slow starting containers, avoiding them 
	  getting killed by the kubelet before they are up and running.
	- If start up probe succeeds then liveness & readiness will be executed.  
	- It fine tunes the dependencies.
	
	ports:
		- name:liveness-port
		containerPort:8080
		hostPort:8080
		
	livenessProbe:
		httpGet:
			path:/healthz
			port:liveness-port
		failureThreshold:1
		periodSeconds:10

	startupProbe:
		httpGet:
			path:/healthz
			port:liveness-port
		failureThreshold:30
		periodSeconds:10

Liveness Probe: (Health of application, if it fails container will be restarted)
		- The livenessProbe is used to determine the health of the applicaiton running inside the pod.
		- If livenessProbe fails the container will be restarted.
	Ex: Due to some reason like memory leaks in applicaiton or due to high CPU/RAM usage the applicaiton
		    is not responding to our requests, Then in this situation livenessProbe will fail and it will restart the pod.
		- we can define livenessProbe with 3 endpoints 
	
		livenessProbe:
			httpGet:
				path: /test
				port: 8080 
			initialDelaySeconds: 5
			timeoutSeconds: 2 
			periodSeconds: 4

		livenessProbe:
			tcpSocket:
				port: 8080 
			initialDelaySeconds: 5
			timeoutSeconds: 10 
			periodSeconds: 4
			successThreshold: 5
			failuerThreshold: 3
		
		Name port: 	
			ports: 
				- name: line-port	
				  conateinerPort: 8080
				  hostPost: 8080
				  
			livenessProbe:
				httpGet:
					path: /test
					port: line-port

	initialDelaySeconds: After the container started the number of seconds to wait before triggering the probe.	
	timeoutSeconds: Number of seconds after which the probe times out - default/minimun 1 second 
	periodSeconds: How frequently to perform the probe. Default value is 1 second.
	successThreshold: minimum consecutive successes for probe to be considered successful after having failed.
	failureThreshold: no. of times probe tries before giving up/restarting (liveness & startup) 
	                  or marking as Unready 
		
Readiness Probe: (appn is ready to accept traffic, wen this prob fails traffic will be halted to this pod and when successful the load balancer traffic is allowed to this pod )
	- A Readiness Probe is used to determine if a application is in a ready state to accept the traffic.
 	- When this probe fails traffic will be halted to this pod and when successful the load balancer traffic is allowed to this pod.	  
	  
	- Sometimes, applications are temporarily unable to serve traffic. 
	  For example, an application might need to load large data or 
	  configuration files during startup or depend on external services 
	  after startup. In such cases, you don’t want to kill the application,
	  but you don’t want to send it requests either
	  
	- Readiness and liveness probes can be used in parallel for the same container. 
	  Using both can ensure that traffic does not reach a container that is not 
	  ready for it, and the containers are restarted when they fail.
		
	- Readiness probe can be defines wtith 3 endpoints same as livenessProbe.	
		
Type of probes endpoint: 
Type of http status codes 
1. Informational responses ( 100 – 199 ): don't indicate an error. You don't need to troubleshoot these errors.
-> server has received the request and is continuing to process it, about the progress of the request.

2. Successful responses ( 200 – 299 ): no troubleshooting, 
-> the server has successfully processed the request and sent a valid response back to the client.

3. Redirects ( 300 – 399): make sure that the endpoint that the probe is trying to access is 
   correct and that there are no issues with the DNS.

4. Client errors ( 400 – 499 ): indicate that the client has made a mistake in the request.
-> Check the logs of the container to determine the cause of the error.
-> $$ kubectl logs my-pod my-container --since=5m   (logs of my-container in my-pod from last 5min )
-> Make sure that the probe is configured correctly and that the container is listening on the correct port.
-> 404 Not Found: the requested resource was not found on the server. This can occur if 
   the client sends a request for a resource that does not exist or is not accessible.
-> 401 Unauthorized (client needs to authenticate itself ), 403 Forbidden (client does not have permission to access the resource).

5. Server errors ( 500 – 599 ):  indicate that the server was unable to fulfill the request due to an error on the server side.
-> Check the logs of the container to determine the cause of the error. (kubectl logs my-pod my-container --since=5m)
-> Make sure that the container is running and that there are no issues with the container's dependencies.
-> server is not responding, permission error, entire appn is crashed all servers are not workng, permission 
   issue wit firewall, debug these errors, memory exceeded this type of error is coming,delete cookies.
-> 500 Internal Server Error, which indicates that an unexpected error occurred on the server while processing the request. 
   This can occur if there is a bug in the server code, or if there is an issue with the server's infrastructure.
-> 503 Service Unavailable, which indicates that the server is temporarily unable to handle the request due to high traffic 
   or maintenance. This can occur if the server is overloaded or if it is undergoing maintenance.   
-> 502 Bad Gateway, which indicates that there was an issue with a gateway or proxy server.
-> 504 Gateway Timeout, which indicates that a gateway or proxy server did not receive a timely response from an upstream server.

1. HTTP/HTTPS endpoint (httpGet)
	- For successful replay we need to get 2XX series replay
	- For failure we need to get 4XX or 5XX http error. 
	- the kubelet sends an HTTP GET request to the server that is running in the container and listening on port 8080.   
	- If the handler for the server's /healthz path returns a success code, the kubelet considers the container to be alive and healthy. 	  
	- If the handler returns a failure code, the kubelet kills the container and restarts it.
			
		HTTP probes fields
			livenessProbe:
				httpGet: 
					path: /mail/u/0/#inbox
					port: 8080
					host: localhost
		host: Host name to connect, the default will be IP of pod.
		path: Path to access on the HTTP server/host.
		port: the port to access on the container.
		
2. TCP endpoints 		
	- In this case kubelet will try to open a tcp socket the port in the container and it will check
	  whether the applicaiton is accessable on that port.
	- For successful replay we need to get 2XX series replay	
		livenessProbe:
			tcpSocket:
			port: 9090
		
3. EXEC commands		
	- Run a shell command, on execution in pod’s shell context and considered failed if the execution returns
	  any result code different from 0 (zero).
			  
		livenessProbe:
			exec:
				command:
					- cat 
					- /etc/temp
			
		
------------------------------------------------------------------------------------------------------------------------------------------------------------
 If project has 20-25 worker nodes it is required to create namespace.
------------------------------------------------------------------------------------------------------------------------------------------------------------
Namespaces:
-> Kubernetes namespace is an abstraction to support multiple virtual clusters of k8s objects on the same physical cluster.  
-> Each namespace has its own set of resources, such as pods and services, and can be used to isolate resources within a cluster. 
   Namespaces can also be used to control access to resources, by assigning different roles and permissions to users and groups within each namespace.
	
	Namespaces main functionalities.
		- Namespaces are virtual cluster on top of physical cluster.		
		- Namespaces work at cluster level.
		- within same namespace by default a pod can communicate with other pod.
		- Namespaces provides a logical seperation between environments.
		- Namespaces are only hidden from each other but are not fully isolated, 
		  one service in a NS can talk to another serive in another NS 
		  using fullname like service/object name followed by namespace name 
		
	List namespaces
		sudo kubectl get ns  (or)  sudo kubectl get namespace

Types of default namespaces
	1. default:
		- resources will be created under default if we don’t specify any other namespace
		- if we don’t give namespace then the entire cluster resides in default.
	2. kube-system: 
		- This namespace is for objects created by the kubernetes system.
		- To isolate the Kubernetes master/control plane components (API server, ectd, scheduler, and controller). 	
	3. kube-public: 
		- The objects/resources in this namespace are available or accessable by all. 
		- The objects in this namespace will be public.
		- We never create resources in this namespace until and unless resource should be visible and readable 
                  publicly throughout the cluster.	 
	4. kube-node-lease:
		- This namespace for the lease objects associated with each node which improves the performance 
                  of the node heartbeats as the cluster scales.
        - This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send 
		  heartbeats so that the control plane can detect node failure.
		- In Kubernetes, a kube-node-lease is a Lease object that is created by the kubelet on each node. 
		  The kube-node-lease is used to indicate that a node is still "alive" and responsive. The kube-node-lease 
                  is created in the kube-node-lease namespace with the name of the node.

	Create a namespace
		sudo kubectl create namespace <name_of_namespace>
		
	List objects of perticular NS
		sudo kubectl get -n <name_of_namespace> <object_type>
		sudo kubectl get -n test pod
		
	Create object in perticular NS
		matadata: 
			name: ......
			namespace: <name_of_namespace> 
				(or) 	
		sudo kubectl apply -f <filename> -n <name_of_namespace>	
		
	To list the pods in NS
		sudo kubectl get pods -n <ns>
	
	To list the pods in all NS
		sudo kubectl get pods --all-namespaces
		
Namespaces can also be created and attached to pod through config file  

	- To create namespace.yml
	
		apiVersion: v1
		kind: Namespace
		metadata:
			name: test-ns
			
	- To define namespace in deployment config yaml
	
		apiVersion: apps/v1
		kind: Deployment
		metadata:
			name: nginx-deployment
			namespace: test-ns
		spec:
			selector:
				matchLabels:
					app: nginx
				minReadySeconds: 5
			template:
				metadata:
				labels:
					app: nginx
				spec:
					containers:
						- name: nginx
						image: nginx:1.14.2
						ports:
						- containerPort: 80

Q. Can we have 2 applications in a single namespace?
-> Yes, it is possible to have multiple applications within a single namespace in Kubernetes. A namespace is a logical boundary for resources in a cluster, 
   and it is used to group resources together and provide isolation between them. Each namespace has a unique name and can contain a variety of 
   resources such as pods, services, and deployments.
-> Having multiple applications in a single namespace can be useful for scenarios where the applications are closely related or share similar resources. 
   This can make it easier to manage and organize the resources, and also allows for better resource allocation between the applications. 
   However, it is important to keep in mind that resources in a namespace are not completely isolated from resources in other namespaces, 
   so it's important to have a proper access control and security measures in place.
-> Additionally, having multiple applications in a single namespace can also increase complexity, and make it harder to troubleshoot 
   issues and monitor resources. This could be especially true if the applications are not closely related, have different scaling 
   requirements and have different security requirements. In such cases it is better to have them in different namespaces.

------------------------------------------------------------------------------------------------------------------------------------------------------------
How to create/schedule pods in a particular worker node?
	1. Node selector. 
	2. Afinity and Anti-Afinity. 
        3. Taint and tolerations.

1. Node selector: 
	- Node Selector is a way to bind the pod to a particular worker node, whose label 
		  match the nodeSelector lables.
	- Logical expresions type of selection cannot be achived by nodeSelector.		
STEP 1: 		
	- list nodes 
		sudo kubectl get nodes 
			
	- Get details of nodes 
		sudo kubectl describe nodes 
			
	- Get details of particular node / nodes 
		sudo kubectl describe node <node_name>
				
	- list pods with nodes details 
		sudo kubectl get pods -o wide					
STEP 2: 	
	- Create a label for the node 
		sudo kubectl label node <node_name> <label_key>=<label_value>
		sudo kubectl label node ip-172-31-46-206 env=test	
STEP 3: 	
	- use nodeSelector field in spec file 
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   nodeSelector:
				  env: test
				  
STEP 4: create the object form STEP 3 and describe the node to check whether the pod is create in the particular node.	


** NOTE:  Hard and Soft rules for affinity & anti-affinity 
	
			- Hard rule - requiredDuringSchedulingIgnoredDuringExecution - With “hard” affinity, 
			  users can set a precise rule that should be met in order for a Pod to be scheduled on a node.
			  
			- Soft rule - preferredDuringSchedulingIgnoredDuringExecution - Using “soft” affinity, you can
			  ask the scheduler to try to run the set of Pod in availability zone XYZ, but if it’s 
			  impossible, allow some of these Pods to run in the other Availability Zone.


2. Node Afinity and Anti-affinity:
a. Node affinity: allows us to schedule the pods to specific nodes with conditional expressions.	
	- Creating pods accross different availability zones to improve the applicaiton availability (resilience).
	- Allocating pods to nodes based on memory-intensive mode. Means create pod based on CPU and RAM availability in worker nodes.
		  
	- Environment based worker nodes.	

	apiVersion: v1
	kind: Pod
	metadata:
	   name: node-selector
	   labels:
		  env: test
	spec:
	   containers:
		  - name: nginx
			image: nginx
	   affinity:
		  nodeAffinity:
			 requiredDuringSchedulingIgnoredDuringExecution:
			    nodeSelectorTerms: 
					- matchExpressions: 
						- key: env
						  operator: in 
						  values:
							- test
							- prod

b. Anti-Affinity (Inter-pod affinity)
	- We can define whether a given Pod should or should not be scheduled onto a particular node based on labels.
	
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone=V
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone=R
  containers:
  - name: with-pod-affinity
    image: registry.k8s.io/pause:2.0

This example defines one Pod affinity rule and one Pod anti-affinity rule. 
The Pod affinity rule uses the "hard" requiredDuringSchedulingIgnoredDuringExecution, 
while the anti-affinity rule uses the "soft" preferredDuringSchedulingIgnoredDuringExecution.

The affinity rule says that the scheduler can only schedule a Pod onto a node if the node is in the same zone as one or more existing Pods 
with the label security=S1. 
More precisely, the scheduler must place the Pod on a node that has the topology.kubernetes.io/zone=V label, 
as long as there is at least one node in that zone that currently has one or more Pods with the Pod label security=S1.

The anti-affinity rule says that the scheduler should try to avoid scheduling the Pod onto a node 
that is in the same zone as one or more Pods with the label security=S2. More precisely, 
the scheduler should try to avoid placing the Pod on a node that has the topology.kubernetes.io/zone=R label 
if there are other nodes in the same zone currently running Pods with the Security=S2 Pod label.

NOTE: 	
	Naming convetion of kubernetes objects name ?
		- contain no more than 253 characters.
		- contain only lowercase alphanumeric characters, '-' or '.'
		- start with an alphanumeric character.
		- end with an alphanumeric character.
		
3. Taint and Tolerations:
	- Taints are used to repel pods from specific nodes.
	- We apply a taint to node which tells the scheduler to repel pods from that worker node.
	- Only pods consisting of toleration for that taint will be created in that worker node.
        - Taint effect defines how nodes with taint react to pods.

	- NoSchedule taint: means that unless a pod has matching toleration, k8s won't be able to schedule the pod 
	  to tainted node.
	- PreferNoSchedule - Kubernetes will try to not schedule the pod onto the node.
	- NoExecute taint: To delete/evict all the pods except some required pods  .
	- The master node is already tainted by:
			taints:
			  - effect: NoSchedule
			key: node-role.kubernetes.io/master

	
	STEP 1: 
			- How to taint a node 
				sudo kubectl taint nodes <node_name> <taint_key>=<taint_value>:<taint_effect>
				sudo kubectl taint nodes ip-172-31-46-206 special=true:Schedule
		
		Note: taint key and value can be anything user defined.
		      taint_effect is from kubernetes - NoSchedule, PreferNoSchedule, NoExecute. 
			  1. NoSchedule - Kubernetes will not schedule the pod onto that node.
			  -> The noschedule taint is the more restrictive of the two. When a node has a noschedule taint, 
                             no pods can be scheduled on that node, regardless of whether they have tolerations for the taint.
			  2. PreferNoSchedule - Kubernetes will try to not schedule the pod onto the node.
			  -> The preferNoSchedule taint is less restrictive. When a node has a preferNoSchedule taint, 
			     pods will be scheduled on the node if they have a toleration for the taint, but pods without 
                             the toleration will be scheduled on other nodes if possible. This allows for some control over which 
			     pods are scheduled on a node, but does not completely prevent pods from being scheduled on the node.
			  3. NoExecute - the pod will be evicted/deleted from the node (if it is already running on the node), 
				         and will not be scheduled onto the node (if it is not yet running on the node).
			  
	STEP 2: 
			- Adding toleration to pod
			
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    value: "true"
                    operator: "Equal"
                    effect: "Schedule"					
					
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    operator: "Exists"
                    effect: "Schedule"	
					
		NOTE: The default value for operator is Equal.

			  - A toleration "matches" a taint if the keys are the same and the effects are the same, and:

			  - the operator is Exists (in which case no value should be specified), or
				the operator is Equal and the values are equal.

Q. how to run pod on particular node?
-> $$  kubectl label nodes node-1 environment=production
-> In the pod spec, you will need to include the nodeSelector field and set it to the label you want to match.
-> $$ kubectl create -f pod.yaml   
   (In some cases, you may want to use affinity and anti-affinity rules to ensure that pods are scheduled on 
   specific nodes or avoid other pods.)

------------------------------------------------------------------------------------------------------------------------------------------------------------
Resource quotas and limitaions:
-> When several users or teams share a cluster with a fixed number of nodes, 
   there is a concern that one team could use more than its fair share of resources.

Q. What is control group/ C-group?
-> It allows for resource management and isolation of processes & to specify limits and constraints on system resources.(CPU, memory, and I/O)
-> Kubernetes, where it is used to ensure that containers are given the resources they need to run, without overloading the host system. 
   In this context, each container runs in its own cgroup, and K8s can set limits and constraints on the resources used by the container.

The administrator creates one ResourceQuota for each namespace.
	-Resource quotas are the way of restricting teams to use the resources such CPU, RAM and Disk.
	-We can also control the number of objects created in namespace using Object count quota.
        -Resource quotas always works at namespace level.
	
	To check quotas of a namespace 
		sudo kubectl describe ns <namespace_name>
		
	OBJECT COUNT QUOTA 
		List of resources that we can limit via object count quotas
			count/persistentvolumeclaims
			count/services
			count/secrets
			count/configmaps
			count/replicationcontrollers
			count/deployments.apps
			count/replicasets.apps
			count/statefulsets.apps
			count/jobs.batch
			count/cronjobs.batch
					
			apiVersion: v1
			kind: ResourceQuota
			metadata:
				name: object-counts
			spec:
				hard:
				  pods: 2  
				  
	Resource quota on CPU and memory

kubectl describe quota --namespace=myspac
	
apiVersion: v1
kind: Pod
metadata:
   name: node-selector
   labels:
	  env: test
spec:
   containers:
	  - name: nginx
		image: nginx
   resources:
	  requests:
		 memory: "128Mi"	
		 cpu: 0.5
	  limits:
		 memory: "256Mi"
		 cpu: 1
		 
============================================================================================================================
RBAC (Role based access control)
	#Accounts in Kubernetes
	#Roles in kubernetes
	#Binding of roles 

RBAC - Role-Based access control is a method of regulating access to the kubernetes resources based on roles of a account.		
	   
	Key points in RBAC 
			Subject: Users, Groups or service accounts 
			Resources: Kubernetes objects which need to be operated with RBAC
			Verbs: The rules/operations which we want to do with the resources. ("get", "list", "watch", "create", "update", "patch", "delete")
	
	There are 2 types of accounts in kubernetes
		1. USER ACCOUNT
			It is used to allow us, humans to access the kubernetes cluster.
			
		2. SERVICE ACCOUNT
			-It is used to access the API server by other tools and also components inside the clusetr.
			-API server is responsible for such authentication process.
			-If any application running inside a pod or ouside the cluster can access kubernetes cluster 
			 using a service account.
			
			-when a service account is created it first creates a token and keeps that token in a secret 
			 object and token can be used by mounting the secret object. 
			-Secret object is linked to the service account.

Kubectl create sa <sa-name>-n namespace

			apiVersion: v1
			kind: Pod
			metadata:
			   name: monitoring-pod
			spec:
				serviceAccountName: myaccount
				conateiners:
				   .....
			
Role & ClusterRole
	-Role and ClusterRole contains set of rules to access and modify kubernetes resources. (There are no deny rules)
	-Role is used to set the permissions/rules within a namespace.
	-ClusterRole is cluster wide permissions which is a non-namespaces object.
		
Create a Role 
command line-->kubectl create role my-role --verb=get --verb=get,list --verb=watch --resource=pods

or

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
   name: my-role
   namespace: test
rules:
   - apiGroups:
        - "*"
     resources:
        - pods
        - services
		- deployments	
     verbs:
		- get
		- list

Create a ClusterRole
  cli-->kubectl create clusterrole my-cluster-role --verb=get,list,watch --resource=pods,deployments
(or)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
   name: my-cluster-role
rules:
   - apiGroups:
        - "*"
     resources:
        - pods
		- deployments	
     verbs:
		- get
		- list
     		- watch

Role Binding and ClusterRole Binding
	Role and ClusterRole binding is used to attach the Role to a service account.
	In Role binding we can bind role to a ClusterRole within a namespace.
	
	RoleBinding 
	
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
   name: my-role
   namespace: test
roleRef: 
   apiGroup: rbac.authorization.k8s.io 
   kind: Role
   name: my-role
subjects:
   - kind: ServiceAccount
     name: myaccount
     namespace: test

-----------------------------------------------------------------------------------------------------------------------------------------
Assignment:
		Create Role and ClusterRole with binding to a service account.
		Resource quotas CPU Memory and count.
-----------------------------------------------------------------------------------------------------------------------------------------
Kubernetes volumes: 
-> Volumes in kubernetes is a kind of directory which is accessable to the containers in the pods.		
 
Types of volumes 
1. emptyDir: It is a volume that is created by default when a pod is first assigned to node, 
	     It remains active untill the pod is running.
2. hostPath: Mounts the files or directories from there host nodes.
3. CLoud storages 
	- gcePersistentDisk
	- awsElasticBlockStore (aws EBS)
	- azureDiskVolume
			
1. Persistent Volumes:
-> It is a piece of storage which can be attached to the pod.
   These are always a cluster level resource not namespace level resource which are maintained by kubernetes.
			
	Create a persistent volume
		apiVersion: v1
		kind: PersistentVolume
		metadata:
			 name: my-pv
			 labels:
			 volume: test	
		spec: 
				storageClassName: local (or) cloud
				capacity:
					  storage: 100Gi
				accessModes:
 					  - ReadWriteOnce
				hostPath:
					path: "<path of the host>"(tmp/data)(cloud volume)
					(path to which we are creating the volumes)
					
2. Persistent Volume Claim
-> This object is used to cliam and mount the required amount of space of volume to pod.
-> Ensure that the PVC has the same storageClassName as the PV.
				apiVersion: v1
				kind: PersistentVolumeClaim
				metadata:
				   name: my-pvc
				spec: 
					storageClassName: local or cloud
					accessModes:
 					   - ReadWriteOnce
					resources:
						resource:
						   storage: 2Gi
					selector:
						matchLanels:
							volume: test				
		  Attach it to pod 	
		  
		    apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   volumes:
				  - name: sample-volume
                    persistentVolumeClaim: 
				 claimName: my-pvc
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    value: "true"
                    operator: "Equal"
                    effect: "NoSchedule"	

Q. Terminology used in PVC claim?
-> It is a request for storage resources that are provisioned by a storage class. used to provide persistent storage for pods and other resources in a cluster.
-> The following are the key terms and concepts used in the context of PVCs:
1. Persistent Volume (PV): A PV is a piece of storage that has been provisioned by an administrator or provisioner. 
   PVs are independent of pods and can be bound to one or more PVCs.
2. Storage Class: A way to provision and manage storage resources for pods and other resources.
3. Claim: A claim is a request for storage resources by a user. A PVC is a specific type of claim that requests storage resources that are backed by a PV.
4. Volume: A volume is a specific instance of a PV that is associated with a PVC. A volume is created when a PVC is bound to a PV.
5. Volume Mode: A volume mode defines whether a volume should be mounted as a file or as a block device.
6. Access Modes: Access Modes are used to specify how a volume can be accessed by a pod. The access modes include ReadWriteOnce, ReadOnlyMany and ReadWriteMany.
7. Volume Binding: The process of associating a PVC with a PV, creating a volume and making it available to a pod.
8. Volume Provisioning: The process of allocating storage resources to a PV.
9. Volume Expansion: The process of increasing the size of a PV after it has been provisioned.
10.Volume Snapshot: A snapshot of a volume's contents at a specific point in time, which can be used to create a new PV.

Q. what is storage class?
-> A way to provision and manage storage resources for pods and other resources. 
  (defines the properties of a specific storage solution such as the performance, durability, and availability of the storage) 
-> A storage class can be used to create a Persistent Volume Claim(PVC) which can be used to request storage resources from the cluster. 
   Once a storage class is created, it can be used by multiple pods to create PVCs that use that class.
-> A storage class can define different parameters such as:
1. The type of storage (e.g. SSD{solid state drive}, HDD{hrd disk drive})
2. The level of replication or availability
3. The performance characteristics (e.g. IOPS, throughput)
4. The size of the storage

Q. What does reclaim policy delete mean?
-> For dynamically provisioned PersistentVolumes, the default reclaim policy is "Delete". This means that a dynamically 
   provisioned volume is automatically deleted when a user deletes the corresponding PersistentVolumeClaim. 
-> This automatic behavior might be inappropriate if the volume contains precious data.

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Q: how do you delete an object file from spec file?
-> To delete an object from a Kubernetes Spec file, you can use the "kubectl edit" command to 
   open the spec file in a text editor, and then manually remove the object definition from the file.
-> how you can delete an object called "my-object" from a spec file called "my-spec.yaml":
   $$ kubectl edit -f my-spec.yaml
-> Alternatively, you can use "kubectl patch" command to update the spec file and remove the object.
   $$ kubectl patch -f my-spec.yaml -p '{"op": "remove", "path": "/spec/template/spec/containers/0"}'

Q. What are helm charts in k8s?  helm:package manager, charts: bundle of yaml playbooks
-> Helm helps you manage Kubernetes applications — Helm Charts help you define, install, manage the lifecycle and upgrade even the most complex 
   Kubernetes application. 
-> Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.
-> Helm uses a packaging format called charts, which are collections of Kubernetes manifests that describe the resources needed to run an application.
-> Charts include:
1. A file called Chart.yaml that contains metadata about the chart, such as its name, version, and description.
2. A file called values.yaml that contains default configuration values for the chart.
3. A directory called templates that contains the Kubernetes manifests for the resources defined in the chart.
-> Why do we use Helm charts in Kubernetes?
1. Helm is a package manager for Kubernetes that makes it easy to take applications and services that are either highly repeatable 
   or used in multiple scenarios and deploy them to a typical K8s cluster.
2. Charts can be used to deploy applications, libraries, or any other type of workload on a Kubernetes cluster.

Q. What is heapster in K8s?
-> Heapster is a Kubernetes add-on that enables monitoring and performance analysis of Kubernetes clusters. 
   It provides a simple way to collect and aggregate cluster-level metrics and events from various sources 
   such as Kubernetes API server, Kubelet and cAdvisor. 
-> Heapster is typically used to track the following metrics: CPU and memory usage of pods and nodes, network traffic,
   Disk usage, Pod and container uptime & Clusture-level events.
** monitoring: cAdvisor, Heapster, Prometheus, Grafana, daemonset

Q. How to renew certificates in k8s?
-> You can renew your certificates manually at any time with the kubeadm certs renew command. This command performs the renewal 
   using CA (or front-proxy-CA) certificate and key stored in /etc/kubernetes/pki . 
   After running the command you should restart the control plane Pods.

Q. What is Prometheus and Grapana and how to setup it with k8s?
-> Prometheus is a monitoring and alerting toolkit that collects and stores metrics data from various 
   sources and allows querying and analysis of that data in real-time. 
-> Prometheus will have Prometheus server(Retreival, TSDB, HTTP server) which scrapes and stores time series data.

Setup prometheus on Kubernetes:
-> Most efficient way to deploy proetheus in k8s is using Helm charts. 
-> We can write manifest file and deploy but not preferred. Most efficient helm charts maintned by helm community. 
   No need to create YAML file helm chart already al required info.
-> Helm chharts-Prometheus chart: fork this prject hich as prometheus chart to your account.
-> commands used:
	$$ helm repo add prometheus-community https://prometheus-community.github.i...
	$$ helm install prometheus prometheus-community/prometheus
	$$ kubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-ext
	$$ minikube service prometheus-server-ext (you can use this interface)
	$$ kubectl get pods/svc/ (rquired prometeus pods will be created)
	$$ 

-> Grafana, on the other hand, is a data visualization and analytics platform that integrates with Prometheus 
   and other data sources to create dashboards, alerts, and other visualizations for monitoring and analysis.

Q. what is pdb in k8? PodDisruptionBudget
-> PDB (PodDisruptionBudget) to ensure that your pods are not terminated or evicted unless it's absolutely necessary. 
   A PodDisruptionBudget (PDB) defines the number of replicas of a pod that must be available at any given time for 
   some reason such as upgrades or routine maintenance work on the Kubernetes nodes.
-> ECK manages a default PDB per Elasticsearch resource.
* (A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled 
   for some reason such as upgrades or routine maintenance work on the Kubernetes nodes). 

Q. Cordon and drain:
-> Cordon will mark the node as unschedulable.
-> Drain makes it unschedulable and evicts the pods.Safely evict all of your pods from a node before you perform maintenance
   on the node.If any maintenance activity is done on the node then pods cant be evict and moved to another node manually. 
   So drain will be done on the node where it safely evicts the pods to another node, then cordon is used. Make sure there 
   are 2 or 3 nodes available in the cluster with sufficient capacity to maintain the pods.
   
Q. What is kubectl drain? or what is maintainance acheived in K8s?
-> The kubectl drain command is used to safely evict all the pods running on a specific node and move them to other available nodes in the cluster. 
   This command is typically used when a node needs to be taken down for maintenance or replacement.
   $$ kubectl drain <node-name>
   
Q. What is federated clusture in  K8s?
-> A federated Kubernetes cluster is a collection of multiple, geographically dispersed K8s clusters 
   that are managed as a single, unified cluster. This allows you to spread your workloads across multiple 
   clusters and regions, providing high availability and disaster recovery capabilities.
-> Federated clusters are useful for several use cases such as:
1. Multi-cloud deployments: You can deploy your applications across multiple cloud providers for better availability and disaster recovery.
2. Global scale: You can deploy your applications in multiple regions for better performance and availability for your end-users.
3. Hybrid cloud: You can manage your on-premises and cloud-based clusters as a single entity, allowing you to easily move resources between them.

Q. What are operators in k8s?
-> Operators in Kubernetes (k8s) are a way to extend the functionality of the platform by introducing 
   custom resources and custom controllers. They are used to automate and simplify the management of 
   complex, stateful applications in k8s.
-> Operators uses the Kubernetes API to manage the lifecycle of a specific application or service.
   It does this by creating, configuring, and managing Kubernetes resources on behalf of the user.
-> Operators can be used to automate tasks such as scaling, backup and restore, and upgrades of stateful applications.
*  Two main components:
1. The Operator controller (custom controller): It watches for changes to the custom resource and 
   responds by creating, updating, or deleting the corresponding Kubernetes resources.
2. The custom resource definition (CRD): It defines the desired state of the application or service. 
   Used by operator controller in order to determine what actions to take to bring the application or service to the desired state. 
-> A values file is used in conjunction with an Operator to specify the configuration of an application managed by the Operator.
-> Some common values used in Operators include:
1. Image versions: Specifying the version of the images used to run the application
2. Resource requirements: Defining the resource requirements for the application, such as CPU and memory limits
3. Feature flags: Enabling or disabling specific features of the application
4. Configuration settings: Setting values for various configuration options, such as log level or security settings

Q. How do u secure etcd?: to protect sensitive data and prevent unauthorized access. 
1. Authentication: Enable client authentication to ensure that only authorized clients can access etcd. (client certificates or client authentication tokens)
2. Encryption: Enable transport encryption to ensure that all communication between clients and etcd is secure. (Transport Layer Security (TLS)
3. Authorization: Use etcd's built-in role-based access control (RBAC) to ensure that clients can only access the keys and values.
4. Network segmentation: Use firewall rules and network policies to limit access to etcd to only authorized clients.
5. Backup and restore: Regularly backup etcd data and have a disaster recovery plan in place in case of data loss.
6. Monitor and Audit: Monitor etcd for any suspicious activity and have audit logs to have visibility on etcd's access.
7. Update and patch: Keep etcd up-to-date to ensure that any security vulnerabilities are patched.

Q. Secrets in k8s?
-> A secret is a way to store sensitive information, such as passwords, tokens, and certificates, in a secure and managed way. 
   Secrets are stored as key-value pairs and can be used by pods and other resources in a cluster.
-> The data stored in secrets is encrypted at rest, and only decrypted by the API server when it is sent to a pod.
*  There are two ways to create a secret in Kubernetes:
1. Manually create a secret using the "kubectl create secret" command.
2. Automatically create a secret by a configuration file and use "kubectl apply -f file.yaml" command.
-> Once created, you can use the "kubectl get secrets" command to list all the secrets in a namespace and "kubectl describe secret" to see the details of a secret.
-> Secrets can be consumed by pods in several ways, such as:
*  Mounting secrets as files in a pod's filesystem.
*  Using secrets as environment variables in a pod's containers.
*  Using secrets in configmaps
	$$ kubectl create secret
	$$ kubectl apply -f file.yaml
	$$ kubectl get secrets
	$$ kubectl describe secret
	
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. What is Quadrant and grains in k8s?
-> Quadrant: It is responsible for managing the scheduling of workloads in a cluster.
## Quadrant refers to the four main areas of focus in Kubernetes (K8s):
1. Workloads & Scheduling: managing the deployment and scaling of containers.
2. Data Services: storage and management of data.
3. Networking: communication between different components in a K8s cluster.
4. Security: ensuring the secure operation of the K8s cluster.
-> Grain: is a unit of measurement used to describe the resources required by a workload in a cluster. 
-> Grain refers to the size and granularity of the components within each quadrant.
   It is used by the scheduler to determine where to place a workload based on the available resources in the cluster.
   
Q. How to delegate a task to another host? 
-> Important to consider security and authorization and the communication is secure while deligating task.
1. Identify the host that will perform the task: the host that has the necessary resources (CPU and memory, and is available to perform the task)
2. Communicate the task to the host: Once the host is identified, the task needs to be communicated to the host. (sending an HTTP request, message queue or  custom protocol) 
3. Confirm receipt of the task: The host receiving the task should confirm receipt of the task to ensure.
4. Perform the task: The host performs the task using the resources available to it. (send updates or status information back)
5. Send the result: Once the task is completed, the host sends the result back to the original host. (https request, message qe, custom protocol)
6. Confirm receipt of the result: The original host should confirm receipt of the result to ensure.

Common #kubernetes pod related errors and ways to mitigate 🤔 ☸
 
➡ ImagePullBackOff
This error appears when #k8s isn't able to retrieve the image for one of the #containers of the Pod.
There are three common culprits:
     ✅ The image name is invalid 
     ✅ You specified a non-existing tag for the image.
     ✅ The image that you're trying to retrieve belongs to a private  registry and the cluster doesn't have credentials to access it.
The first two cases can be solved by correcting the image name and tag.
For the last, one should add the credentials to your private registry in a Secret and reference it in the Pods

➡ RunContainerError
The error appears when the container is unable to start before application
    Common causes:
     ✅ Mounting a not-existent volume such as ConfigMap or Secrets 
     ✅ Mounting a read-only volume as read-write
More detailed aspect can be found by describing the 'failed' pod

➡ CrashLoopBackOff
If the container can't start, then #Kubernetes shows the CrashLoopBackOff message as a status.
Usually, a container can't start when:
  ✅   There's an error in the application that prevents it from starting.
  ✅   You misconfigured the container.
  ✅   The Liveness probe failed too many times.
The "CrashLoopBackOff" error in Kubernetes is usually caused by:
Incorrect container image
Container image pulled from a private registry that is not accessible to the cluster
Insufficient resources (e.g. memory or CPU)
Application crashing or exiting with non-zero status code
Incorrect environment variables
Misconfigured liveness/readiness probes
Persistent disk failure
Networking issues between containers.

➡ Pods in a Pending state
Assuming that the  scheduler component is running fine, here are the causes:
  ✅   The cluster doesn't have enough resources such as CPU and memory to run the Pod.
  ✅   The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota.
  ✅   The Pod is bound to a Pending PersistentVolumeClaim.

The best option is to inspect the Events section in the "kubectl describe"

-----------------------------------------------------------------------------------------------------------------------------------------
Crash loop backoff error:  is a Kubernetes state representing a restart loop that is happening in a Pod: 
      A container in the Pod is started, but crashes and is then restarted, over and over again. 
      Kubernetes will wait an increasing back-off time between restarts to give you a chance to fix the error.

*Container is failing beacuse of images is not present.

Types production errors in k8s:
*  Steps to help troubleshoot(commands) and fix the error:
1. Check the container's logs: The first step is to check the logs of the container that is crashing. 
    "kubectl logs <pod-name>". The logs may provide information about the cause of the crash.
-> $$ "kubectl exec" command to run a command in a specific container in a pod.
-> $$ "kubectl get events" command to view recent events related to a pod and the resources in its namespace.
2. Check resource limits: Make sure that the container has enough resources allocated to it. 
    "kubectl describe pod <pod-name>" to check the resource limits and usage.
3. Check the readiness and liveness probes: The readiness and liveness probes can be used to detect and 
   respond to container failures. Check the configuration of these probes and make sure they are configured correctly.
4. Check the environment variables: Make sure that the environment variables passed to the container are 
   correct and that the container has access to the necessary resources.
5. Check the container image: Make sure that the container image is the correct version and that it is not corrupt. 
   Try pulling the image again to ensure it is up to date.
6. Check the application code: If none of the above steps resolve the issue, check the application code 
   for any bugs or issues that may be causing the crash.
7. Last option is to delete the pod and let it recreate.

-> Use Prometheus and Grafana for monitoring and alerting, it helps to get more visibility on resource usage and performance metrics.
-> Use "Kubernetes events" to get more information about the state of the pod.
-> Use the Kubernetes dashboard, which provides a web-based interface for monitoring and managing pods and other resources in the cluster.

Q. What common commands for performing maintenance activities on a Kubernetes node?
-> Upgrading the node's operating system and Kubernetes software:
	$$ sudo apt-get update && sudo apt-get upgrade -y
-> Monitoring the node's resource usage:
	$$ kubectl describe nodes <node-name>
	$$ kubectl top nodes <node-name>
-> Checking for and removing unused or evicted pods:
	$$ kubectl get pods --all-namespaces
	$$ kubectl delete pod <pod-name> --namespace=<namespace>
-> Running a security scan on the node:
	$$ docker run -it --net host --pid host --cap-add audit_control -e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST -v 
    /var/lib:/var/lib -v 
    /var/run/docker.sock:/var/run/docker.sock -v 
    /usr/lib/systemd:/usr/lib/systemd -v /etc:/etc --label 
    docker_bench_security docker/docker-bench-security
-> Backing up important data:
	$$ kubectl get pods --all-namespaces -o json > pod-backup.json

------------------------------------------------------------------------------------------------------------------------------------------------
Q. Deployment Strategies:
-> Deployment strategies define how you want to deliver your software. Organizations follow different deployment strategies based on 
   their business model. Some may choose to deliver software that is fully tested, and others may want their users to provide feedback 
   and let their users evaluate under development features (for example, beta releases). In the following section we will talk about 
   various deployment strategies.
1. In-Place Deployments
2. Blue/Green Deployments
3. Canary Deployments
4. Linear Deployments
5. All-at-once Deployments

1. In-Place Deployments:
-> In this strategy, the deployment is done when the application on each instance in the deployment group is stopped, 
   the latest application revision is installed, and the new version of the application is started and validated. 
-> You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. 
-> In-place deployments can be all-at-once, assuming a service outage, or done as a rolling update. AWS CodeDeploy and AWS Elastic Beanstalk offer 
   deployment configurations for one at a time, half at a time and all at once. These same deployment strategies for in-place 
   deployments are available within blue/green deployments.

2. Blue/Green Deployments  or Red/black deployment:
-> It is a technique for releasing applications by shifting traffic between two identical environments
   running differing versions of the application. 
-> Blue/green deployments help you minimize downtime during application updates mitigating risks surrounding downtime 
   and rollback functionality. 
-> Blue/green deployments enable you to launch a new version (green) of your application alongside the old version (blue), 
   and monitor and test the new version before you reroute traffic to it, rolling back on issue detection.

3. Canary Deployments:
-> Traffic is shifted in two increments. A canary deployment is a blue/green strategy that is more risk-averse, 
   in which a phased approach is used. 
-> This can be two step or linear in which new application code is deployed and exposed for trial, and upon acceptance 
   rolled out either to the rest of the environment or in a linear fashion.

4. Linear Deployments: 
-> Linear deployments mean that traffic is shifted in equal increments with an equal number of minutes between each increment. 
   You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number 
   of minutes between each increment.

5. All-at-once Deployments
-> All-at-once deployments mean that all traffic is shifted from the original environment to the replacement environment all at once.

---------------------------------------------------------------------------------------------------------------------------------------------------------
Q. what is meant by high availability of clusture?
-> High availability (HA) in a cluster refers to the ability of the cluster to continue functioning properly even 
   in the event of a failure or outage. This means that if one node or component of the cluster goes down, 
   the other nodes can take over and continue providing the service without interruption. 
-> A highly available cluster typically includes multiple nodes, with each node running the same service and a 
   mechanism for automatically detecting and responding to failures. This can be achieved by using a load balancer, 
   and various type of replication.

-------------------------------------------------------------------------------------------------------------------------------------
Q. Types of errors and how will u troubleshoot them in k8s?
-> There are several types of errors that can occur in a Kubernetes cluster, and the troubleshooting process will depend on the specific error. 
  Here are a few common types of errors and some general troubleshooting steps for each:

1. Pod and container errors: These include issues such as "CrashLoopBackOff" and "ErrImagePull". 
   To troubleshoot these errors, check the container logs using "kubectl logs", check the resource limits using "kubectl describe pod", 
   and check the readiness and liveness probes.
2. Networking errors: These include issues such as "Connection refused" and "Error forwarding ports". To troubleshoot these errors, 
   check the network configuration, check for firewall rules blocking the traffic, check the pod-to-pod and service-to-service communication.
3. Scheduling errors: These include issues such as "No resources available" and "NodeNotReady". To troubleshoot these errors, 
   check the resource usage on the nodes, check the resource limits and requests for pods, check the node conditions.
4. Deployment errors: These include issues such as "Failed to create deployment" and "Invalid configuration". To troubleshoot these errors, 
   check the deployment configuration, check the environment variables, check the image version, check the application code.
5. Persistent Volume Claim errors: These include issues such as "PVC not found" or "Unable to bind PVC". To troubleshoot these errors, 
   check the PVC and PV status, check the storage class, check the access modes, check the storage capacity.
6. Authentication and Authorization errors: These include issues such as "Forbidden" and "Unauthorized". To troubleshoot these errors, 
   check the service account, check the role-based access control, check the authentication and authorization method.
-> It's worth noting that the above steps are general troubleshooting steps and may not apply to every error. Also, many times multiple 
   issues can happen together, so it's important to look into every possible point.

-----------------------------------------------------------------------------------------------------------------------------------------------
Q. Types of controller managers?
-> Types of controller managers are:
 1) endpoints controller, 
 2) service accounts controller, 
 3) node controller, 
 4) namespace controller,  
 5) replication controller, 
 6) token controller.

---------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How to access the application using kunernetes?
-> To access an application deployed in a Kubernetes cluster, you need to use a Service resource. A Service defines the network 
   access to a set of replicas of an application. There are several types of Services in Kubernetes, including ClusterIP, 
   NodePort, LoadBalancer, and ExternalName, each with different features and use cases.
-> Here are the general steps to access an application using Kubernetes:
1. Create a Deployment for the application: This defines the desired state for the replicas of the application.
2. Create a Service for the Deployment: This defines the network access to the replicas, such as the IP address, port, and target selector.
3. Access the application using the Service's IP address and port: You can access the application by connecting 
   to the Service's IP address and port from within the cluster or from outside the cluster. 
   The exact method depends on the type of Service you created.
-> For example, if you created a ClusterIP Service, you can access the application by connecting to the Service's IP address 
   and port within the cluster. If you created a LoadBalancer Service, you can access the application by connecting to the 
   external IP address assigned by the cloud provider.
   
Q. What is secret ashmaps in k8s ?
-> A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 

Q. How to debug the nodes in k8s ?
	1.Reconfiguring a kubeadm cluster.
	2.Changing the Container Runtime on a Node from Docker Engine to containerd. ...
	3.Generate Certificates Manually.
	4.Reconfigure a Node's Kubelet in a Live Cluster.
	5.Using NodeLocal DNSCache in Kubernetes Clusters.
	6.Verify Signed Kubernetes Artifacts.
	
Q. How to troubleshoot the k8s cluster ?
	1.Administer a Cluster. Reconfiguring a kubeadm cluster. ...
	2.Use a User Namespace With a Pod.
	3.Monitoring, Logging, and Debugging. Troubleshooting Applications. ...
	4.Horizontal Pod Autoscaling. ...
	5.Job with Pod-to-Pod Communication. ...
	6.Deploy and Access the Kubernetes Dashboard.
	7.Use a SOCKS5 Proxy to Access the Kubernetes API.

Q. How we can do the storages in k8s ?
-> A Kubernetes cluster stores all its data in etcd.
	
Q. How Kubernetics manifest looks like?  Explain Kubernetics manifest yml file.
-> Kubernetes manifests ( .yaml or .json) are used to create, modify and delete Kubernetes resources such as pods, deployments, services or ingresses
   pp requirement is push to azure container and  deploy through Aks,

Q. you are able to deploy app to K8 service A developer is saying the app is down. How will You trouble shoot app other than using probes How to trouble shoot the AKS cluster?
kubectl get pods [check status of pods]
kubectl get logs application[wil be chceked for dependencies]
kubectl describe pod <pod name>[to view appl related to pods]
kubectl describe service <service name>[to view service related to pods]
the pod vwill be deleted and restarted

Q. Explain Kubectl cmds?
kubectl run <pod-name> --image=<image-name>
kubectl create cm <configmap-name> --from-literal=<key>=<value>
kubeclt create cm <configmap-name> --from-file=<file-name>
kubectl create deployment <deployment-name> --image=<image-name>
kubectl scale deployment <deployment-name> --replicas=<new-replica-count>
kubectl delete pod <pod name>

Q. Some person is hitting the k8 cluster from browser how to  make the app visible to him.
one need to have access to the cluster then he can use kubectl commands to access the cluster
kubectl -n kube-system edit service kubernetes-dashboard
kubectl -n kube-system get services

Q. In K8 manifest file we have to use  secrets in k8 namespace how to  inject to k8secrets into ns
$ kubectl create namespace testns1
[namespace/testns1 created]
$ kubectl create secret generic test-secret-1 --from-literal=username=test-user --from-literal=password=testP@ssword -n testns1
[secret/test-secret-1 created]

apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
	
---------------------------------------------------------------------------------------------------------------------------------------------------
HTTP AND HTTPS:
-> HTTP (Hypertext Transfer Protocol) and HTTPS (Hypertext Transfer Protocol Secure) are both protocols used for transmitting data over the internet.
-> HTTP is the foundation of data communication on the web and is used to request and transmit data 
   between servers and clients. HTTP is a stateless protocol, which means that each request and response 
   is independent of previous requests and responses.
-> HTTPS is a more secure version of HTTP. It uses a combination of HTTP and SSL/TLS (Secure Sockets Layer/Transport Layer Security) 
   protocols to encrypt data transmitted between a client and a server, which helps to prevent unauthorized access, 
   data interception, and other security threats.
-> HTTPS is identified by the "https://" prefix in a website's URL, and it uses port 443 instead of port 80, which is used by HTTP. 
   HTTPS is commonly used for transmitting sensitive information such as passwords, credit card numbers, and other personal data.
-> In summary, HTTP is the basic protocol for transmitting data on the web, while HTTPS is a more secure version of HTTP that 
   uses encryption to protect data. When transmitting sensitive information, it's important to use HTTPS to ensure that 
   the data is protected from unauthorized access and interception.

** SSL (Secure Sockets Layer) and TLS (Transport Layer Security) are cryptographic protocols that are used to 
   secure communication over the internet. SSL was the original protocol developed by Netscape in the mid-1990s, while TLS is its successor.
-> SSL/TLS is used to encrypt data transmitted between a client and a server, providing confidentiality, integrity, 
   and authentication, which helps to prevent unauthorized access, data interception, and other security threats. 
   It is commonly used for securing sensitive data such as passwords, credit card numbers, and other personal information.
-> When a client and a server establish an SSL/TLS connection, they go through a process called the handshake, which includes the following steps:
1. The client sends a hello message to the server, which includes the client's SSL/TLS version, 
   a list of supported cipher suites, and other information.
2. The server responds with a hello message, which includes the server's SSL/TLS version, the chosen cipher suite, and other information.
3. The server sends its SSL/TLS certificate to the client, which is used to authenticate the server.
4. The client verifies the server's certificate and, if the certificate is valid, generates a random key that 
   will be used to encrypt the data transmitted between the client and the server.
5. The client encrypts the random key using the server's public key, which is included in the server's 
   certificate, and sends the encrypted key to the server.
6. The server decrypts the random key using its private key and uses the key to encrypt data transmitted between the client and the server.

-> Once the SSL/TLS connection is established, all data transmitted between the client and the server is encrypted 
   and can only be decrypted by the intended recipient.
-> In summary, SSL/TLS is a cryptographic protocol used to secure communication over the internet. 
   It provides confidentiality, integrity, and authentication, which helps to prevent unauthorized access, 
   data interception, and other security threats.

Q. if u delete a pod what else will delete?
-> When a Kubernetes pod is deleted, the following resources associated with the pod will also be deleted:
1. Containers: The containers running inside the pod will be terminated and deleted.
2. Volumes: Any volumes attached to the pod will be unmounted and deleted.
3. Services: If the pod is part of a service, the service endpoints will be updated to remove the pod.
4. Replication controllers or deployments: If the pod is part of a replication controller or deployment, 
   the controller will create a new pod to maintain the desired number of replicas.
5. Stateful sets: If the pod is part of a stateful set, the stateful set controller will create a new pod 
   with the same name to maintain the desired state.
6. Daemon sets: If the pod is part of a daemon set, the daemon set controller will create a new pod on the 
   same node to maintain the desired state.
7. Jobs or CronJobs: If the pod is part of a job or CronJob, the job controller will create a new pod to complete the job.
-> It's important to note that deleting a pod in Kubernetes is not the same as stopping or killing a process on 
   a traditional server. Kubernetes will attempt to replace the pod to maintain the desired state of the application.

Q. how u manage kubernetes using jenkins pipeline?
-> There are several ways to manage Kubernetes using Jenkins pipeline. Here are the high-level steps to manage Kubernetes using Jenkins pipeline:
1. Install the Kubernetes plugin: To manage Kubernetes using Jenkins pipeline, you need to install 
   the Kubernetes plugin. This plugin provides integration between Jenkins and Kubernetes, allowing 
   you to create, deploy, and manage Kubernetes resources directly from your Jenkins pipeline.
2. Create a Kubernetes configuration file: To interact with your Kubernetes cluster, you need to create 
   a configuration file that contains the necessary details of your Kubernetes cluster, such as the API 
   server URL, certificate authority, and token or username/password for authentication.
3. Define Kubernetes deployment in Jenkinsfile: In your Jenkinsfile, define the Kubernetes deployment, 
   service, or other resources that you want to create or update. This can be done using the Kubernetes 
   plugin's DSL, which provides a set of functions to interact with the Kubernetes API.
4. Use Kubernetes plugin steps in pipeline: Use Kubernetes plugin's pipeline steps such as kubernetesDeploy, 
   kubernetesRollback, kubernetesScale, kubernetesVerifyDeploy, and others to manage Kubernetes resources such 
   as deployments, services, ConfigMaps, and Secrets.
5. Build and deploy: Build and deploy your application to Kubernetes using your Jenkins pipeline. 
   Your pipeline should include the steps to build your application code, containerize the application, 
   push the container image to a container registry, and deploy the application to Kubernetes using the Kubernetes plugin.
-> By using Jenkins pipeline to manage Kubernetes, you can automate the deployment and management of your 
   applications in Kubernetes, ensuring that your applications are always up-to-date and running as expected. 
   This approach also allows you to easily manage multiple Kubernetes clusters and deploy applications to different 
   environments, such as development, testing, and production.
